<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Stochastic Explorations Using MCMC | An Introduction to Bayesian Thinking</title>
  <meta name="description" content="Chapter 8 Stochastic Explorations Using MCMC | An Introduction to Bayesian Thinking" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Stochastic Explorations Using MCMC | An Introduction to Bayesian Thinking" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="StatsWithR/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Stochastic Explorations Using MCMC | An Introduction to Bayesian Thinking" />
  
  
  

<meta name="author" content="Merlise Clyde" />
<meta name="author" content="Mine Çetinkaya-Rundel" />
<meta name="author" content="Colin Rundel" />
<meta name="author" content="David Banks" />
<meta name="author" content="Christine Chai" />
<meta name="author" content="Lizzy Huang" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-model-choice.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html"><i class="fa fa-check"></i><b>1</b> The Basics of Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayes-rule"><i class="fa fa-check"></i><b>1.1</b> Bayes’ Rule</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#sec:bayes-rule"><i class="fa fa-check"></i><b>1.1.1</b> Conditional Probabilities &amp; Bayes’ Rule</a></li>
<li class="chapter" data-level="1.1.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#sec:diagnostic-testing"><i class="fa fa-check"></i><b>1.1.2</b> Bayes’ Rule and Diagnostic Testing</a></li>
<li class="chapter" data-level="1.1.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayes-updating"><i class="fa fa-check"></i><b>1.1.3</b> Bayes Updating</a></li>
<li class="chapter" data-level="1.1.4" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayesian-vs.-frequentist-definitions-of-probability"><i class="fa fa-check"></i><b>1.1.4</b> Bayesian vs. Frequentist Definitions of Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion"><i class="fa fa-check"></i><b>1.2</b> Inference for a Proportion</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion-frequentist-approach"><i class="fa fa-check"></i><b>1.2.1</b> Inference for a Proportion: Frequentist Approach</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion-bayesian-approach"><i class="fa fa-check"></i><b>1.2.2</b> Inference for a Proportion: Bayesian Approach</a></li>
<li class="chapter" data-level="1.2.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#effect-of-sample-size-on-the-posterior"><i class="fa fa-check"></i><b>1.2.3</b> Effect of Sample Size on the Posterior</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#frequentist-vs.-bayesian-inference"><i class="fa fa-check"></i><b>1.3</b> Frequentist vs. Bayesian Inference</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#frequentist-vs.-bayesian-inference-1"><i class="fa fa-check"></i><b>1.3.1</b> Frequentist vs. Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>2</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#continuous-variables-and-eliciting-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Continuous Variables and Eliciting Probability Distributions</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#from-the-discrete-to-the-continuous"><i class="fa fa-check"></i><b>2.1.1</b> From the Discrete to the Continuous</a></li>
<li class="chapter" data-level="2.1.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#elicitation"><i class="fa fa-check"></i><b>2.1.2</b> Elicitation</a></li>
<li class="chapter" data-level="2.1.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#conjugacy"><i class="fa fa-check"></i><b>2.1.3</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#three-conjugate-families"><i class="fa fa-check"></i><b>2.2</b> Three Conjugate Families</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#inference-on-a-binomial-proportion"><i class="fa fa-check"></i><b>2.2.1</b> Inference on a Binomial Proportion</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-gamma-poisson-conjugate-families"><i class="fa fa-check"></i><b>2.2.2</b> The Gamma-Poisson Conjugate Families</a></li>
<li class="chapter" data-level="2.2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#sec:normal-normal"><i class="fa fa-check"></i><b>2.2.3</b> The Normal-Normal Conjugate Families</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals-and-predictive-inference"><i class="fa fa-check"></i><b>2.3</b> Credible Intervals and Predictive Inference</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-conjugate-priors"><i class="fa fa-check"></i><b>2.3.1</b> Non-Conjugate Priors</a></li>
<li class="chapter" data-level="2.3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>2.3.2</b> Credible Intervals</a></li>
<li class="chapter" data-level="2.3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#predictive-inference"><i class="fa fa-check"></i><b>2.3.3</b> Predictive Inference</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html"><i class="fa fa-check"></i><b>3</b> Losses and Decision Making</a>
<ul>
<li class="chapter" data-level="3.1" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#bayesian-decision-making"><i class="fa fa-check"></i><b>3.1</b> Bayesian Decision Making</a></li>
<li class="chapter" data-level="3.2" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#loss-functions"><i class="fa fa-check"></i><b>3.2</b> Loss Functions</a></li>
<li class="chapter" data-level="3.3" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#working-with-loss-functions"><i class="fa fa-check"></i><b>3.3</b> Working with Loss Functions</a></li>
<li class="chapter" data-level="3.4" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#minimizing-expected-loss-for-hypothesis-testing"><i class="fa fa-check"></i><b>3.4</b> Minimizing Expected Loss for Hypothesis Testing</a></li>
<li class="chapter" data-level="3.5" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#sec:bayes-factors"><i class="fa fa-check"></i><b>3.5</b> Posterior Probabilities of Hypotheses and Bayes Factors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html"><i class="fa fa-check"></i><b>4</b> Inference and Decision-Making with Multiple Parameters</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:normal-gamma"><i class="fa fa-check"></i><b>4.1</b> The Normal-Gamma Conjugate Family</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#conjugate-prior-for-mu-and-sigma2"><i class="fa fa-check"></i><b>4.1.1</b> Conjugate Prior for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="4.1.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#conjugate-posterior-distribution"><i class="fa fa-check"></i><b>4.1.2</b> Conjugate Posterior Distribution</a></li>
<li class="chapter" data-level="4.1.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#marginal-distribution-for-mu-student-t"><i class="fa fa-check"></i><b>4.1.3</b> Marginal Distribution for <span class="math inline">\(\mu\)</span>: Student <span class="math inline">\(t\)</span></a></li>
<li class="chapter" data-level="4.1.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#credible-intervals-for-mu"><i class="fa fa-check"></i><b>4.1.4</b> Credible Intervals for <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="4.1.5" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:tapwater"><i class="fa fa-check"></i><b>4.1.5</b> Example: TTHM in Tapwater</a></li>
<li class="chapter" data-level="4.1.6" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#section-summary"><i class="fa fa-check"></i><b>4.1.6</b> Section Summary</a></li>
<li class="chapter" data-level="4.1.7" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#optional-derivations"><i class="fa fa-check"></i><b>4.1.7</b> (Optional) Derivations</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-MC"><i class="fa fa-check"></i><b>4.2</b> Monte Carlo Inference</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>4.2.1</b> Monte Carlo Sampling</a></li>
<li class="chapter" data-level="4.2.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#monte-carlo-inference-tap-water-example"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Inference: Tap Water Example</a></li>
<li class="chapter" data-level="4.2.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#monte-carlo-inference-for-functions-of-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Monte Carlo Inference for Functions of Parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#summary"><i class="fa fa-check"></i><b>4.2.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-predictive"><i class="fa fa-check"></i><b>4.3</b> Predictive Distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#prior-predictive-distribution"><i class="fa fa-check"></i><b>4.3.1</b> Prior Predictive Distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#tap-water-example-continued"><i class="fa fa-check"></i><b>4.3.2</b> Tap Water Example (continued)</a></li>
<li class="chapter" data-level="4.3.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sampling-from-the-prior-predictive-in-r"><i class="fa fa-check"></i><b>4.3.3</b> Sampling from the Prior Predictive in <code>R</code></a></li>
<li class="chapter" data-level="4.3.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#posterior-predictive"><i class="fa fa-check"></i><b>4.3.4</b> Posterior Predictive</a></li>
<li class="chapter" data-level="4.3.5" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#summary-1"><i class="fa fa-check"></i><b>4.3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-reference"><i class="fa fa-check"></i><b>4.4</b> Reference Priors</a></li>
<li class="chapter" data-level="4.5" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-Cauchy"><i class="fa fa-check"></i><b>4.5</b> Mixtures of Conjugate Priors</a></li>
<li class="chapter" data-level="4.6" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-MCMC"><i class="fa fa-check"></i><b>4.6</b> Markov Chain Monte Carlo (MCMC)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Testing with Normal Populations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#sec:known-var"><i class="fa fa-check"></i><b>5.1</b> Bayes Factors for Testing a Normal Mean: variance known</a></li>
<li class="chapter" data-level="5.2" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#comparing-two-paired-means-using-bayes-factors"><i class="fa fa-check"></i><b>5.2</b> Comparing Two Paired Means using Bayes Factors</a></li>
<li class="chapter" data-level="5.3" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#sec:indep-means"><i class="fa fa-check"></i><b>5.3</b> Comparing Independent Means: Hypothesis Testing</a></li>
<li class="chapter" data-level="5.4" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#inference-after-testing"><i class="fa fa-check"></i><b>5.4</b> Inference after Testing</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html"><i class="fa fa-check"></i><b>6</b> Introduction to Bayesian Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:simple-linear"><i class="fa fa-check"></i><b>6.1</b> Bayesian Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#frequentist-ordinary-least-square-ols-simple-linear-regression"><i class="fa fa-check"></i><b>6.1.1</b> Frequentist Ordinary Least Square (OLS) Simple Linear Regression</a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#bayesian-simple-linear-regression-using-the-reference-prior"><i class="fa fa-check"></i><b>6.1.2</b> Bayesian Simple Linear Regression Using the Reference Prior</a></li>
<li class="chapter" data-level="6.1.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:informative-prior"><i class="fa fa-check"></i><b>6.1.3</b> Informative Priors</a></li>
<li class="chapter" data-level="6.1.4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:derivations"><i class="fa fa-check"></i><b>6.1.4</b> (Optional) Derivations of Marginal Posterior Distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="6.1.5" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#marginal-posterior-distribution-of-beta"><i class="fa fa-check"></i><b>6.1.5</b> Marginal Posterior Distribution of <span class="math inline">\(\beta\)</span></a></li>
<li class="chapter" data-level="6.1.6" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#marginal-posterior-distribution-of-alpha"><i class="fa fa-check"></i><b>6.1.6</b> Marginal Posterior Distribution of <span class="math inline">\(\alpha\)</span></a></li>
<li class="chapter" data-level="6.1.7" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#marginal-posterior-distribution-of-sigma2"><i class="fa fa-check"></i><b>6.1.7</b> Marginal Posterior Distribution of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="6.1.8" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#joint-normal-gamma-posterior-distributions"><i class="fa fa-check"></i><b>6.1.8</b> Joint Normal-Gamma Posterior Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:Checking-outliers"><i class="fa fa-check"></i><b>6.2</b> Checking Outliers</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#posterior-distribution-of-epsilon_j-conditioning-on-sigma2"><i class="fa fa-check"></i><b>6.2.1</b> Posterior Distribution of <span class="math inline">\(\epsilon_j\)</span> Conditioning On <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="6.2.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#implementation-using-bas-package"><i class="fa fa-check"></i><b>6.2.2</b> Implementation Using <code>BAS</code> Package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:Bayes-multiple-regression"><i class="fa fa-check"></i><b>6.3</b> Bayesian Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#the-model"><i class="fa fa-check"></i><b>6.3.1</b> The Model</a></li>
<li class="chapter" data-level="6.3.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#data-pre-processing"><i class="fa fa-check"></i><b>6.3.2</b> Data Pre-processing</a></li>
<li class="chapter" data-level="6.3.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#specify-bayesian-prior-distributions"><i class="fa fa-check"></i><b>6.3.3</b> Specify Bayesian Prior Distributions</a></li>
<li class="chapter" data-level="6.3.4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#fitting-the-bayesian-model"><i class="fa fa-check"></i><b>6.3.4</b> Fitting the Bayesian Model</a></li>
<li class="chapter" data-level="6.3.5" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#posterior-means-and-posterior-standard-deviations"><i class="fa fa-check"></i><b>6.3.5</b> Posterior Means and Posterior Standard Deviations</a></li>
<li class="chapter" data-level="6.3.6" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#credible-intervals-summary"><i class="fa fa-check"></i><b>6.3.6</b> Credible Intervals Summary</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#summary-2"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html"><i class="fa fa-check"></i><b>7</b> Bayesian Model Choice</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#sec:BIC"><i class="fa fa-check"></i><b>7.1</b> Bayesian Information Criterion (BIC)</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#definition-of-bic"><i class="fa fa-check"></i><b>7.1.1</b> Definition of BIC</a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#backward-elimination-with-bic"><i class="fa fa-check"></i><b>7.1.2</b> Backward Elimination with BIC</a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#coefficient-estimates-under-reference-prior-for-best-bic-model"><i class="fa fa-check"></i><b>7.1.3</b> Coefficient Estimates Under Reference Prior for Best BIC Model</a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#other-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Other Criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#sec:BMU"><i class="fa fa-check"></i><b>7.2</b> Bayesian Model Uncertainty</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#model-uncertainty"><i class="fa fa-check"></i><b>7.2.1</b> Model Uncertainty</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#calculating-posterior-probability-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Calculating Posterior Probability in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>7.3</b> Bayesian Model Averaging</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#visualizing-model-uncertainty"><i class="fa fa-check"></i><b>7.3.1</b> Visualizing Model Uncertainty</a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#bayesian-model-averaging-using-posterior-probability"><i class="fa fa-check"></i><b>7.3.2</b> Bayesian Model Averaging Using Posterior Probability</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#coefficient-summary-under-bma"><i class="fa fa-check"></i><b>7.3.3</b> Coefficient Summary under BMA</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#summary-3"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html"><i class="fa fa-check"></i><b>8</b> Stochastic Explorations Using MCMC</a>
<ul>
<li class="chapter" data-level="8.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#stochastic-exploration"><i class="fa fa-check"></i><b>8.1</b> Stochastic Exploration</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#markov-chain-monte-carlo-exploration"><i class="fa fa-check"></i><b>8.1.1</b> Markov Chain Monte Carlo Exploration</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#other-priors-for-bayesian-model-uncertainty"><i class="fa fa-check"></i><b>8.2</b> Other Priors for Bayesian Model Uncertainty</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#zellners-g-prior"><i class="fa fa-check"></i><b>8.2.1</b> Zellner’s <span class="math inline">\(g\)</span>-Prior</a></li>
<li class="chapter" data-level="8.2.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#bayes-factor-of-zellners-g-prior"><i class="fa fa-check"></i><b>8.2.2</b> Bayes Factor of Zellner’s <span class="math inline">\(g\)</span>-Prior</a></li>
<li class="chapter" data-level="8.2.3" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#kids-cognitive-score-example"><i class="fa fa-check"></i><b>8.2.3</b> Kid’s Cognitive Score Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#r-demo-on-bas-package"><i class="fa fa-check"></i><b>8.3</b> R Demo on <code>BAS</code> Package</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#the-uscrime-data-set-and-data-processing"><i class="fa fa-check"></i><b>8.3.1</b> The <code>UScrime</code> Data Set and Data Processing</a></li>
<li class="chapter" data-level="8.3.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#bayesian-models-and-diagnostics"><i class="fa fa-check"></i><b>8.3.2</b> Bayesian Models and Diagnostics</a></li>
<li class="chapter" data-level="8.3.3" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#posterior-uncertainty-in-coefficients"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Uncertainty in Coefficients</a></li>
<li class="chapter" data-level="8.3.4" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#prediction"><i class="fa fa-check"></i><b>8.3.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#decision-making-under-model-uncertainty"><i class="fa fa-check"></i><b>8.4</b> Decision Making Under Model Uncertainty</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#model-choice"><i class="fa fa-check"></i><b>8.4.1</b> Model Choice</a></li>
<li class="chapter" data-level="8.4.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#prediction-with-new-data"><i class="fa fa-check"></i><b>8.4.2</b> Prediction with New Data</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#summary-4"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Thinking</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="stochastic-explorations-using-mcmc" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Stochastic Explorations Using MCMC<a href="stochastic-explorations-using-mcmc.html#stochastic-explorations-using-mcmc" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter, we will discuss stochastic explorations of the model space using Markov Chain Monte Carlo method. This is particularly usefull when the number of models in the model space is relatively large. We will introduce the idea and the algorithm that we apply on the kid’s cognitive score example. Then We will introduce some alternative priors for the coefficients other than the reference priors that we have been focused on. We will demonstrate using Markov Chain Monte Carlo on the crime data set to see how to use this stochastic method to explore the model space and how different priors may lead to different posterior inclusion probability of coefficients. Finally, we will summarize decision making strategies under Bayesian model uncertainty.</p>

<div id="stochastic-exploration" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Stochastic Exploration<a href="stochastic-explorations-using-mcmc.html#stochastic-exploration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the last chapter, we explored model uncertainty using posterior probability of each model and Bayesian model averaging based on BIC. We applied the idea on the kid’s cognitive score data set. With 4 predictors, we had <span class="math inline">\(2^4 = 16\)</span> possible models. Since the total number of models is relatively small, it is easy to enumerate all possible models to obtain Bayesian model averaging results. However, in general we often have data sets with large number of variables, which may lead to long computating time via enumeration. In this section, we will present one of the common stochastic methods, Markov Chain Monte Carlo (MCMC), to explore model spaces and implement Bayesian model averaging to estimate quantities of interest.</p>
<div id="markov-chain-monte-carlo-exploration" class="section level3 hasAnchor" number="8.1.1">
<h3><span class="header-section-number">8.1.1</span> Markov Chain Monte Carlo Exploration<a href="stochastic-explorations-using-mcmc.html#markov-chain-monte-carlo-exploration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us assume that we have a pseudo population of possible models that we obtained from all the possible combinations of regression models from the kid’s cognitive score example. We prepare the data set as in Section <a href="introduction-to-bayesian-regression.html#sec:Bayes-multiple-regression">6.3</a> and run <code>bas.lm</code> to obtain posterior probability of each model as we did in Section <a href="bayesian-model-choice.html#sec:BMU">7.2</a>.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="stochastic-explorations-using-mcmc.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data processing</span></span>
<span id="cb122-2"><a href="stochastic-explorations-using-mcmc.html#cb122-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreign)</span>
<span id="cb122-3"><a href="stochastic-explorations-using-mcmc.html#cb122-3" aria-hidden="true" tabindex="-1"></a>cognitive <span class="ot">=</span> <span class="fu">read.dta</span>(<span class="st">&quot;http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta&quot;</span>)</span>
<span id="cb122-4"><a href="stochastic-explorations-using-mcmc.html#cb122-4" aria-hidden="true" tabindex="-1"></a>cognitive<span class="sc">$</span>mom_work <span class="ot">=</span> <span class="fu">as.numeric</span>(cognitive<span class="sc">$</span>mom_work <span class="sc">&gt;</span> <span class="dv">1</span>)</span>
<span id="cb122-5"><a href="stochastic-explorations-using-mcmc.html#cb122-5" aria-hidden="true" tabindex="-1"></a>cognitive<span class="sc">$</span>mom_hs <span class="ot">=</span>  <span class="fu">as.numeric</span>(cognitive<span class="sc">$</span>mom_hs <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb122-6"><a href="stochastic-explorations-using-mcmc.html#cb122-6" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(cognitive) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;kid_score&quot;</span>, <span class="st">&quot;hs&quot;</span>,<span class="st">&quot;IQ&quot;</span>, <span class="st">&quot;work&quot;</span>, <span class="st">&quot;age&quot;</span>) </span>
<span id="cb122-7"><a href="stochastic-explorations-using-mcmc.html#cb122-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-8"><a href="stochastic-explorations-using-mcmc.html#cb122-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Run regression</span></span>
<span id="cb122-9"><a href="stochastic-explorations-using-mcmc.html#cb122-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BAS)</span>
<span id="cb122-10"><a href="stochastic-explorations-using-mcmc.html#cb122-10" aria-hidden="true" tabindex="-1"></a>cog_bas <span class="ot">=</span> <span class="fu">bas.lm</span>(kid_score <span class="sc">~</span> hs <span class="sc">+</span> IQ <span class="sc">+</span> work <span class="sc">+</span> age,</span>
<span id="cb122-11"><a href="stochastic-explorations-using-mcmc.html#cb122-11" aria-hidden="true" tabindex="-1"></a>                <span class="at">prior =</span> <span class="st">&quot;BIC&quot;</span>,</span>
<span id="cb122-12"><a href="stochastic-explorations-using-mcmc.html#cb122-12" aria-hidden="true" tabindex="-1"></a>                <span class="at">modelprior =</span> <span class="fu">uniform</span>(),</span>
<span id="cb122-13"><a href="stochastic-explorations-using-mcmc.html#cb122-13" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> cognitive)</span></code></pre></div>
<p>We will use this example to explore the idea of MCMC and generalize it to regression models with much larger model spaces. To explore the models, we may arrange them by their model sizes, the number of predictors plus the intercept, on the <span class="math inline">\(x\)</span>-axis, and their posterior probabilities on the <span class="math inline">\(y\)</span>-axis.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="stochastic-explorations-using-mcmc.html#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb123-2"><a href="stochastic-explorations-using-mcmc.html#cb123-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-3"><a href="stochastic-explorations-using-mcmc.html#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct data frame for plotting</span></span>
<span id="cb123-4"><a href="stochastic-explorations-using-mcmc.html#cb123-4" aria-hidden="true" tabindex="-1"></a>output <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">model.size =</span> cog_bas<span class="sc">$</span>size, <span class="at">model.prob =</span> cog_bas<span class="sc">$</span>postprobs)</span>
<span id="cb123-5"><a href="stochastic-explorations-using-mcmc.html#cb123-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-6"><a href="stochastic-explorations-using-mcmc.html#cb123-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot model size vs mode posterior probability</span></span>
<span id="cb123-7"><a href="stochastic-explorations-using-mcmc.html#cb123-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> output, <span class="fu">aes</span>(<span class="at">x =</span> model.size, <span class="at">y =</span> model.prob)) <span class="sc">+</span></span>
<span id="cb123-8"><a href="stochastic-explorations-using-mcmc.html#cb123-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb123-9"><a href="stochastic-explorations-using-mcmc.html#cb123-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;model size&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;model posterior probability&quot;</span>)</span></code></pre></div>
<p><img src="08-MCMC-01-stochastic_files/figure-html/model-space-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We could then take a sample from this population of models with replacement (therefore, some models may be selected more than once in this sample). This process could be done using the <code>sample</code> function in R. We hope that the frequency of appearance of a model would be a good approximation of the posterior probability of this model. We use <span class="math inline">\(I(M_j = M_m)\)</span> as the indicator function to indicate that the current model <span class="math inline">\(M_j\)</span> we sample is the model of interest <span class="math inline">\(M_m\)</span>, that is
<span class="math display">\[ I(M_j=M_m) = \left\{\begin{array}{ll} 1, &amp; \text{if $M_j = M_m$} \\ 0, &amp; \text{if $M_j\neq M_m$}\end{array}\right. \]</span></p>
<p>Suppose we are going to sample <span class="math inline">\(J\)</span> models in total, we hope that
<span class="math display" id="eq:MCMC-formula">\[\begin{equation}
p(M_m~|~\text{data}) \approx \frac{\sum_{j=1}^J I(M_j=M_m)}{J} = \sum_{j=1}^J \frac{I(M_j=M_m)}{J}.
\tag{8.1}
\end{equation}\]</span></p>
<p>After all, we would not need to calculate the model posterior probability <span class="math inline">\(P(M_m~|~\text{data})\)</span>. The quantity from the sampling <span class="math inline">\(\displaystyle \sum_{j=1}^J\frac{I(M_j=M_m)}{J}\)</span> would provide a good approximation, which only requires simple counting.</p>
<p>In order to ensure that we would sample models with a probability that is equal to their posterior probability, or in a simpler way, proportional to the marginal likelihood times the prior probability <span class="math inline">\(p(\text{data}~|~M_m)\times p(M_m)\)</span>, we need to design a sampling method that replaces old models with new models when the posterior probability goes up, and keeps the old models when the posterior probability is not improved.</p>
<p>Here, we propose the Metropolis-Hastings algorithm. We start with an initial model <span class="math inline">\(M^{(0)}\)</span>. This could be any model we like in the model space. We start iterating over the entire model space, randomly pick the next model <span class="math inline">\(M^{*(1)}\)</span> and see whether this model improves the posterior probability. We use the notation <span class="math inline">\(M^{*(1)}\)</span> instead of <span class="math inline">\(M^{(1)}\)</span> because we are not sure whether we should include this model in our final sample, or we should consider other models. Therefore, we calculate the ratio between the posterior probability of the two models, the original model <span class="math inline">\(M^{(0)}\)</span>, and the proposed model <span class="math inline">\(M^{*(1)}\)</span>, which turns out to be the posterior odd between the two models
<span class="math display">\[ R=\frac{p(M^{*(1)}~|~\text{data})}{p(M^{(0)}~|~\text{data})}=\text{PO}[M^{*(1)}:M^{(0)}]. \]</span></p>
<p>Our goal is to avoid actually calculating the posterior probability of each model, so we instead would compute <span class="math inline">\(R\)</span> using the Bayes factor and the prior odd of the two models.
<span class="math display">\[ R=\frac{p(M^{*(1)}~|~\text{data})}{p(M^{(0)}~|~\text{data})}=\textit{PO}[M^{*(1)}:M^{(0)}]=\textit{BF}[M^{*(1)}:M^{(0)}]\times \textit{O}[M^{*(1)}:M^{(0)}]. \]</span></p>
<p>If <span class="math inline">\(R\geq 1\)</span>, that means <span class="math inline">\(M^{*(1)}\)</span> will surely improve the posterior probability after seeing the data compared to <span class="math inline">\(M^{(0)}\)</span>. So we would like to include <span class="math inline">\(M^{*(1)}\)</span> into our sample, because <span class="math inline">\(M^{*(1)}\)</span> deserves more occurrence. In this case, we set <span class="math inline">\(M^{*(1)}\)</span> to be <span class="math inline">\(M^{(1)}\)</span>, indicating that it is part of our final sample. However, if <span class="math inline">\(R&lt;1\)</span>, we are not that sure whether <span class="math inline">\(M^{*(1)}\)</span> should be in the sample. But we also do not want to only include models with higher posterior probabilities. Remember that the purpose of this algorithm is to reproduce the frequency of model occurance in the final sample so that the relative frequency of occurrence of each model could be a good proxy of its posterior probability. Even though the proposed model <span class="math inline">\(M^{*(1)}\)</span> has lower posterior probability, we should still have some representatives of this model in our final sample. Hence we set <span class="math inline">\(M^{*(1)}\)</span> to be <span class="math inline">\(M^{(1)}\)</span> with probability <span class="math inline">\(R\)</span>, reflecting the chance that this model would be in our sample is <span class="math inline">\(R\)</span>.</p>
<p>To include <span class="math inline">\(M^{*(1)}\)</span> in the final sample with probability <span class="math inline">\(R\)</span>, we may use a random number generator to generate number between 0 and 1 and see whether this number is larger than <span class="math inline">\(R\)</span>. Or we may set a coin flip with heads showing up with probability <span class="math inline">\(R\)</span>. If the random number is larger than <span class="math inline">\(R\)</span>, or the head shows up using the biased coin, we include this model. Otherwise, we neglect this proposed model and keep on selecting the next model.</p>
<p>Once the first model <span class="math inline">\(M^*{(1))}\)</span> is sampled, we move onto the second model <span class="math inline">\(M^{(2)}\)</span> with the same process. In general, after we have obtained model <span class="math inline">\(M^{(i)}\)</span>, we propose a model <span class="math inline">\(M^{*(i+1)}\)</span> and calculate the ratio of the posterior probabilities of the two models
<span class="math display">\[ R = \frac{p(M^{*(i+1)}~|~\text{data})}{p(M^{(i)}~|~\text{data})}=\textit{BF}[M^{*(i+1)}:M^{(i)}]\times \textit{O}[M^{*(i+1)}:M^{(i)}].\]</span>
If <span class="math inline">\(R\geq 1\)</span>, we unconditionally accept <span class="math inline">\(M^{*(i+1)}\)</span> to be our next model <span class="math inline">\(M^{(i)}\)</span>. If <span class="math inline">\(R&lt;1\)</span>, we accept <span class="math inline">\(M^{*(i+1)}\)</span> to be <span class="math inline">\(M^{(i)}\)</span> with probability <span class="math inline">\(R\)</span>.</p>
<p>After obtaining <span class="math inline">\(J\)</span> models, <span class="math inline">\(M^{(1)}, M^{(2)}, \cdots, M^{(J)}\)</span>, we can count how many models inside this sample is <span class="math inline">\(M_m\)</span>, the model we are interested. Then we use the formula <a href="stochastic-explorations-using-mcmc.html#eq:MCMC-formula">(8.1)</a> to approximate the posterior probability of <span class="math inline">\(M_m\)</span>. These estimated probabilities can be used in model selection or BMA instead of the exact expressions.</p>
<p>We propose model randomly in the above algorithm, i.e., all models are equally likely to be proposed. This can be pretty inefficient if there are lots of models with low probabilities. We may come up with other ways to propose models. For example, we may look at neighboring models of our current model by either adding one predictor that is currently not in the model, or randomly dropping one of the current predictors from the model. We may flip a fair coin to decide whether to add or to drop. This forms a random walk across neighboring models. We may also propose to swap out a current predictor with one that is currently not in the model, which maintains the size of the model. This has the potential to take bigger jumps in the model space. There are other possible moves that can be designed to help move around over the model space. However, we have to be careful to adjust for any potential bias, due to how we propose new models, to ensure that the relative frequency eventually would converge to the posterior probability. In the lecture video, we have demonstrated the Markov Chain Monte Carlo method on the kid’s cognitive score using animation to show how each model was proposed and finally selected.</p>

</div>
</div>
<div id="other-priors-for-bayesian-model-uncertainty" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Other Priors for Bayesian Model Uncertainty<a href="stochastic-explorations-using-mcmc.html#other-priors-for-bayesian-model-uncertainty" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have discussed Bayesian model selection and Bayesian model averaging using BIC. BIC is an asymptotic approximation of the log of marginal likelihood of models when the number of data points is large. Under BIC, prior distribution of <span class="math inline">\(\boldsymbol{\beta}= (\beta_0, \beta_1,\cdots, \beta_p)^T\)</span> is uniformaly flat, which is the same as applying the reference prior on <span class="math inline">\(\boldsymbol{\beta}\)</span> conditioning on <span class="math inline">\(\sigma^2\)</span>. In this section, we will introduce a new conjugate prior distribution, called the Zellner’s <span class="math inline">\(g\)</span>-prior. We will see that this prior leads to simple expressions for the Bayes factor, in terms of summary statistics from ordinary least square (OLS). We will talk about choosing the parameter <span class="math inline">\(g\)</span> in the prior and conduct a sensitivity analysis, using the kid’s cognitive score data that we used in earlier sections.</p>
<div id="zellners-g-prior" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Zellner’s <span class="math inline">\(g\)</span>-Prior<a href="stochastic-explorations-using-mcmc.html#zellners-g-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To analyze the model more conveniently, we still stick with the “centered” regression model. Let <span class="math inline">\(y_1,\cdots,y_n\)</span> to be the observations of the response variable <span class="math inline">\(Y\)</span>. The multiple regression model is</p>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1(x_{1,i}-\bar{x}_1) + \beta_2(x_{2, i}-\bar{x}_2)+\cdots +\beta_p(x_{p, i}-\bar{x}_p)+\epsilon_i, \quad 1\leq i\leq n.\]</span></p>
<p>As before, <span class="math inline">\(\bar{x}_1, \cdots,\bar{x}_p\)</span>, are the sample means of the variables <span class="math inline">\(X_1,\cdots,X_p\)</span>. Since we have centered all the variables, <span class="math inline">\(\beta_0\)</span> is no longer the <span class="math inline">\(y\)</span>-intercept. Instead, it is the sample mean of <span class="math inline">\(Y\)</span> when taking <span class="math inline">\(X_1=\bar{x}_1,\cdots, X_p=\bar{x}_p\)</span>. <span class="math inline">\(\beta_1,\cdots,\beta_p\)</span> are the coefficients for the <span class="math inline">\(p\)</span> variables. <span class="math inline">\(\boldsymbol{\beta}=(\beta_0,\beta_1,\cdots,\beta_p)^T\)</span> is the vector notation representing all coefficients, including <span class="math inline">\(\beta_0\)</span>.</p>
<p>Under this model, we assume</p>
<p><span class="math display">\[ y_i~|~ \boldsymbol{\beta}, \sigma^2~\mathrel{\mathop{\sim}\limits^{\rm iid}}~\textsf{Normal}(\beta_0+\beta_1(x_{1,i}-\bar{x}_1)+\cdots+\beta_p(x_{p,i}-\bar{x}_p), \sigma^2), \]</span>
which is equivalent to
<span class="math display">\[ \epsilon_i~|~ \boldsymbol{\beta}, \sigma^2 ~\mathrel{\mathop{\sim}\limits^{\rm iid}}~\textsf{Normal}(0, \sigma^2). \]</span></p>
<p>We then specify the prior distributions for <span class="math inline">\(\beta_j,\ 0\leq j\leq p\)</span>. Zellner proposed a simple informative conjugate multivariate normal prior for <span class="math inline">\(\boldsymbol{\beta}\)</span> conditioning on <span class="math inline">\(\sigma^2\)</span> as</p>
<p><span class="math display">\[ \boldsymbol{\beta}~|~\sigma^2 ~\sim ~\textsf{Normal}(\boldsymbol{b}_0, \Sigma = g\sigma^2\text{S}_{\bf{xx}}^{-1}). \]</span></p>
<p>Here
<span class="math display">\[ \text{S}_{\bf{xx}} = (\mathbf{X}-\bar{\mathbf X})^T(\mathbf X - \bar{\mathbf X}), \]</span></p>
<p>where the matrix <span class="math inline">\(\mathbf{X}-\bar{\mathbf X}\)</span> is
<span class="math display">\[ \mathbf{X}-\bar{\mathbf X} = \left(\begin{array}{cccc}
|  &amp; |  &amp; \cdots &amp; | \\
X_1-\bar{X}_1 &amp; X_2 - \bar{X}_2 &amp; \cdots &amp; X_p-\bar{X}_p \\
|  &amp; |  &amp; \cdots &amp; |
\end{array}\right) = \left(\begin{array}{cccc}
x_{1, 1} - \bar{x}_1 &amp; x_{2, 1} - \bar{x}_2 &amp; \cdots &amp; x_{p, 1} - \bar{x}_p \\
\vdots &amp; \vdots &amp;  &amp; \vdots \\
x_{1, n} - \bar{x}_1 &amp; x_{2, n} - \bar{x}_2 &amp; \cdots &amp; x_{p, n} - \bar{x}_p
\end{array}
\right).
\]</span></p>
<p>When <span class="math inline">\(p=1\)</span>, this <span class="math inline">\(\text{S}_{\bf{xx}}\)</span> simplifies to <span class="math inline">\(\displaystyle \text{S}_{\text{xx}} = \sum_{i=1}^n(x_{i}-bar{x})^2\)</span>, the sum of squares of a single variable <span class="math inline">\(X\)</span> that we used in Section <a href="introduction-to-bayesian-regression.html#sec:simple-linear">6.1</a>. In multiple regression, <span class="math inline">\(\text{S}_{\bf{xx}}\)</span> provides the variance and covariance for OLS.</p>
<p>The parameter <span class="math inline">\(g\)</span> scales the prior variance of <span class="math inline">\(\boldsymbol{\beta}\)</span>, over the OLS variances <span class="math inline">\(\sigma^2\text{S}_{\bf{xx}}^{-1}\)</span>. One of the advantages of using this prior is ,that it reduces prior elicitation down to two components; the prior mean <span class="math inline">\(\boldsymbol{b}_0\)</span> and the scalar <span class="math inline">\(g\)</span>. We use <span class="math inline">\(g\)</span> to control the size of the variance of the prior, rather than set separate priors for all the variances and covariances (there would be <span class="math inline">\(p(p+1)/2\)</span> such priors for a <span class="math inline">\(p+1\)</span> dimensional multivariate normal distribution).</p>
<p>Another advantage of using Zellner’s <span class="math inline">\(g\)</span>-prior is that it leads to simple updating rules, like all conjugate priors. Moreover, the posterior mean and posterior variance have simple forms. The posterior mean is
<span class="math display">\[ \frac{g}{1+g}\hat{\boldsymbol{\beta}} + \frac{1}{1+g}\boldsymbol{b}_0, \]</span>
where <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the frequentist OLS estimates of coefficients <span class="math inline">\(\boldsymbol{\beta}\)</span>. The posterior variance is
<span class="math display">\[  \frac{g}{1+g}\sigma^2\text{S}_{\bf{xx}}^{-1}. \]</span></p>
<p>From the posterior mean formula, we can see that the posterior mean is a weighted average of the prior mean <span class="math inline">\(\boldsymbol{b}_0\)</span> and the OLS estimate <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. Since <span class="math inline">\(\displaystyle \frac{g}{1+g}\)</span> is strictly less than 1, Zellner’s <span class="math inline">\(g\)</span>-prior shrinks the OLS estimates <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> towards the prior mean <span class="math inline">\(\boldsymbol{b}_0\)</span>. As <span class="math inline">\(g\rightarrow \infty\)</span>, <span class="math inline">\(\displaystyle \frac{g}{1+g}\rightarrow 1\)</span> and <span class="math inline">\(\displaystyle \frac{1}{1+g}\rightarrow 0\)</span>, and we recover the OLS estimate as in the reference prior.</p>
<p>Similarly, the posterior variancc is a shrunken version of the OLS variance, by a factor of <span class="math inline">\(\displaystyle \frac{g}{1+g}\)</span>. The posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> conditioning on <span class="math inline">\(\sigma^2\)</span> is a normal distribution
<span class="math display">\[ \boldsymbol{\beta}~|~\sigma^2, \text{data}~\sim~ \textsf{Normal}(\frac{g}{1+g}\hat{\boldsymbol{\beta}} + \frac{1}{1+g}\boldsymbol{b}_0,\ \frac{g}{1+g}\sigma^2\text{S}_{\bf{xx}}^{-1}). \]</span></p>
</div>
<div id="bayes-factor-of-zellners-g-prior" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Bayes Factor of Zellner’s <span class="math inline">\(g\)</span>-Prior<a href="stochastic-explorations-using-mcmc.html#bayes-factor-of-zellners-g-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Because of this simplicity, Zellner’s <span class="math inline">\(g\)</span>-prior has been widely used in Bayesian model selection and Bayesian model averaging. One of the most popular versions uses the <span class="math inline">\(g\)</span>-prior for all coefficients except the intercept, and takes the prior mean to be the zero vector <span class="math inline">\(\boldsymbol{b}_0 = \bf{0}\)</span>. If we are not testing any hypotheses about the intercept <span class="math inline">\(\beta_0\)</span>, we may combine this <span class="math inline">\(g\)</span>-prior with the reference prior for the intercept <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\sigma^2\)</span>, that is, we set
<span class="math display">\[ p(\beta_0, \sigma^2) \propto \frac{1}{\sigma^2}, \]</span>
and use the <span class="math inline">\(g\)</span>-prior for the rest of the coefficients <span class="math inline">\((\beta_1, \cdots, \beta_p)^T\)</span>.</p>
<p>Under this prior, the Bayes factor for comparing model <span class="math inline">\(M_m\)</span> to the null model <span class="math inline">\(M_0\)</span>, which only has the intercept, is simply
<span class="math display">\[ \textit{BF}[M_m:M_0] = (1+g)^{(n-p_m-1)/2}(1+g(1-R_m^2))^{-(n-1)/2}. \]</span></p>
<p>Here <span class="math inline">\(p_m\)</span> is the number of predictors in <span class="math inline">\(M_m\)</span>, <span class="math inline">\(R_m^2\)</span> is the <span class="math inline">\(R\)</span>-squared of model <span class="math inline">\(M_m\)</span>.</p>
<p>With the Bayes factor, we can compare any two models using posterior odds. For example, we can compare model <span class="math inline">\(M_m\)</span> with the null model <span class="math inline">\(M_0\)</span> by
<span class="math display">\[ \frac{p(M_m~|~\text{data}, g)}{p(M_0~|~\text{data}, g)} = \textit{PO}[M_m:M_0] =  \textit{BF}[M_m:M_0]\frac{p(M_m)}{p(M_0)}. \]</span></p>
<p>Now the question is, how do we pick <span class="math inline">\(g\)</span>? As we see that, the Bayes factor depends on <span class="math inline">\(g\)</span>. If <span class="math inline">\(g\rightarrow \infty\)</span>, <span class="math inline">\(\textit{BF}[M_m:M_0]\rightarrow 0\)</span>. This provides overwhelming evidence against model <span class="math inline">\(M_m\)</span>, no matter how many predictors we pick for <span class="math inline">\(M_m\)</span> and the data. This is the Bartlett’s/Jeffrey-Lindley’s paradox.</p>
<p>On the other hand, if we use any arbitrary fixed value of <span class="math inline">\(g\)</span>, and include more and more predictors, the <span class="math inline">\(R\)</span>-squared <span class="math inline">\(R_m^2\)</span> will get closer and closer to 1, but the Bayes factor will remain bounded. With <span class="math inline">\(R_m^2\)</span> getting larger and larger, we would expect the alternative model <span class="math inline">\(M_m\)</span> would be supported. However, a bounded Bayes factor would not provide overwhelming support for <span class="math inline">\(M_m\)</span>, even in the frequentist approach we are getting better and better fit for the data. This is the information paradox, when the Bayes factor comes to a different conclusion from the frequentist approach due to the boundedness of Bayes factor in the limiting case.</p>
<p>There are some solutions which appear to lead to reasonable results in small and large samples based on empirical results with real data to theory, and provide resolution to these two paradoxes. In the following examples, we let the prior distribution of <span class="math inline">\(g\)</span> depend on <span class="math inline">\(n\)</span>, the size of the data. Since <span class="math inline">\(\text{S}_{\bf{xx}}\)</span> is getting larger with larger <span class="math inline">\(n\)</span>, <span class="math inline">\(g\sigma^2\text{S}_{\bf{xx}}^{-1}\)</span> may get balanced if <span class="math inline">\(g\)</span> also grows relatively to the size of <span class="math inline">\(n\)</span>.</p>
<p><strong>Unit Information Prior</strong></p>
<p>In the case of the unit information prior, we let <span class="math inline">\(g=n\)</span>. This is the same as saying <span class="math inline">\(\displaystyle \frac{n}{g}=1\)</span>. In this prior, we will only need to specify the prior mean <span class="math inline">\(\boldsymbol{b}_0\)</span> for the coefficients of the predicor variables <span class="math inline">\((\beta_1,\cdots,\beta_p)^T\)</span>.</p>
<p><strong>Zellner-Siow Cauchly Prior</strong></p>
<p>However, taking <span class="math inline">\(g=n\)</span> ignores the uncertainty of the choice of <span class="math inline">\(g\)</span>. Since we do not know <span class="math inline">\(g\)</span> a priori, we may pick a prior so that the expected value of <span class="math inline">\(\displaystyle \frac{n}{g}=1\)</span>. One exmaple is the Zellner-Siow cauchy prior. In this prior, we let
<span class="math display">\[ \frac{n}{g}~\sim~ \textsf{Gamma}(\frac{1}{2}, \frac{1}{2}). \]</span></p>
<p><strong>Hyper-<span class="math inline">\(g/n\)</span> Prior</strong></p>
<p>Another example is to set
<span class="math display">\[ \frac{1}{1+n/g}~\sim~ \textsf{Beta}(\frac{a}{2}, \frac{b}{2}), \]</span>
with hyperparameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Since the Bayes factor under this prior distribution can be expressed in terms of hypergeometric functions, this is called the hyper-<span class="math inline">\(g/n\)</span> prior.</p>
</div>
<div id="kids-cognitive-score-example" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Kid’s Cognitive Score Example<a href="stochastic-explorations-using-mcmc.html#kids-cognitive-score-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We apply these priors on the kid’s cognitive score example and compare the posterior probability that each coefficient <span class="math inline">\(\beta_i,\ i = 1,2,3,4\)</span> to be non-zero. We first read in data and store the size of the data into <span class="math inline">\(n\)</span>. We will use this <span class="math inline">\(n\)</span> later, when setting priors for <span class="math inline">\(n/g\)</span>.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="stochastic-explorations-using-mcmc.html#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreign)</span>
<span id="cb124-2"><a href="stochastic-explorations-using-mcmc.html#cb124-2" aria-hidden="true" tabindex="-1"></a>cognitive <span class="ot">=</span> <span class="fu">read.dta</span>(<span class="st">&quot;http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta&quot;</span>)</span>
<span id="cb124-3"><a href="stochastic-explorations-using-mcmc.html#cb124-3" aria-hidden="true" tabindex="-1"></a>cognitive<span class="sc">$</span>mom_work <span class="ot">=</span> <span class="fu">as.numeric</span>(cognitive<span class="sc">$</span>mom_work <span class="sc">&gt;</span> <span class="dv">1</span>)</span>
<span id="cb124-4"><a href="stochastic-explorations-using-mcmc.html#cb124-4" aria-hidden="true" tabindex="-1"></a>cognitive<span class="sc">$</span>mom_hs <span class="ot">=</span>  <span class="fu">as.numeric</span>(cognitive<span class="sc">$</span>mom_hs <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb124-5"><a href="stochastic-explorations-using-mcmc.html#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(cognitive) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;kid_score&quot;</span>, <span class="st">&quot;hs&quot;</span>,<span class="st">&quot;IQ&quot;</span>, <span class="st">&quot;work&quot;</span>, <span class="st">&quot;age&quot;</span>) </span>
<span id="cb124-6"><a href="stochastic-explorations-using-mcmc.html#cb124-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-7"><a href="stochastic-explorations-using-mcmc.html#cb124-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract size of data set</span></span>
<span id="cb124-8"><a href="stochastic-explorations-using-mcmc.html#cb124-8" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(cognitive)</span></code></pre></div>
<p>We then fit the full model using different priors. Here we set model prior to be <code>uniform()</code>, meaning each model has equal prior probability.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="stochastic-explorations-using-mcmc.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BAS)</span>
<span id="cb125-2"><a href="stochastic-explorations-using-mcmc.html#cb125-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Unit information prior</span></span>
<span id="cb125-3"><a href="stochastic-explorations-using-mcmc.html#cb125-3" aria-hidden="true" tabindex="-1"></a>cog.g <span class="ot">=</span> <span class="fu">bas.lm</span>(kid_score <span class="sc">~</span> ., <span class="at">data=</span>cognitive, <span class="at">prior=</span><span class="st">&quot;g-prior&quot;</span>, </span>
<span id="cb125-4"><a href="stochastic-explorations-using-mcmc.html#cb125-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">a=</span>n, <span class="at">modelprior=</span><span class="fu">uniform</span>())</span>
<span id="cb125-5"><a href="stochastic-explorations-using-mcmc.html#cb125-5" aria-hidden="true" tabindex="-1"></a><span class="co"># a is the hyperparameter in this case g=n</span></span>
<span id="cb125-6"><a href="stochastic-explorations-using-mcmc.html#cb125-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-7"><a href="stochastic-explorations-using-mcmc.html#cb125-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Zellner-Siow prior with Jeffrey&#39;s reference prior on sigma^2</span></span>
<span id="cb125-8"><a href="stochastic-explorations-using-mcmc.html#cb125-8" aria-hidden="true" tabindex="-1"></a>cog.ZS <span class="ot">=</span> <span class="fu">bas.lm</span>(kid_score <span class="sc">~</span> ., <span class="at">data=</span>cognitive, <span class="at">prior=</span><span class="st">&quot;JZS&quot;</span>, </span>
<span id="cb125-9"><a href="stochastic-explorations-using-mcmc.html#cb125-9" aria-hidden="true" tabindex="-1"></a>               <span class="at">modelprior=</span><span class="fu">uniform</span>())</span>
<span id="cb125-10"><a href="stochastic-explorations-using-mcmc.html#cb125-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-11"><a href="stochastic-explorations-using-mcmc.html#cb125-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper g/n prior</span></span>
<span id="cb125-12"><a href="stochastic-explorations-using-mcmc.html#cb125-12" aria-hidden="true" tabindex="-1"></a>cog.HG <span class="ot">=</span> <span class="fu">bas.lm</span>(kid_score <span class="sc">~</span> ., <span class="at">data=</span>cognitive, <span class="at">prior=</span><span class="st">&quot;hyper-g-n&quot;</span>, </span>
<span id="cb125-13"><a href="stochastic-explorations-using-mcmc.html#cb125-13" aria-hidden="true" tabindex="-1"></a>                <span class="at">a=</span><span class="dv">3</span>, <span class="at">modelprior=</span><span class="fu">uniform</span>()) </span>
<span id="cb125-14"><a href="stochastic-explorations-using-mcmc.html#cb125-14" aria-hidden="true" tabindex="-1"></a><span class="co"># hyperparameter a=3</span></span>
<span id="cb125-15"><a href="stochastic-explorations-using-mcmc.html#cb125-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-16"><a href="stochastic-explorations-using-mcmc.html#cb125-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical Bayesian estimation under maximum marginal likelihood</span></span>
<span id="cb125-17"><a href="stochastic-explorations-using-mcmc.html#cb125-17" aria-hidden="true" tabindex="-1"></a>cog.EB <span class="ot">=</span> <span class="fu">bas.lm</span>(kid_score <span class="sc">~</span> ., <span class="at">data=</span>cognitive, <span class="at">prior=</span><span class="st">&quot;EB-local&quot;</span>, </span>
<span id="cb125-18"><a href="stochastic-explorations-using-mcmc.html#cb125-18" aria-hidden="true" tabindex="-1"></a>                <span class="at">a=</span>n, <span class="at">modelprior=</span><span class="fu">uniform</span>())</span>
<span id="cb125-19"><a href="stochastic-explorations-using-mcmc.html#cb125-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-20"><a href="stochastic-explorations-using-mcmc.html#cb125-20" aria-hidden="true" tabindex="-1"></a><span class="co"># BIC to approximate reference prior</span></span>
<span id="cb125-21"><a href="stochastic-explorations-using-mcmc.html#cb125-21" aria-hidden="true" tabindex="-1"></a>cog.BIC <span class="ot">=</span> <span class="fu">bas.lm</span>(kid_score <span class="sc">~</span> ., <span class="at">data=</span>cognitive, <span class="at">prior=</span><span class="st">&quot;BIC&quot;</span>, </span>
<span id="cb125-22"><a href="stochastic-explorations-using-mcmc.html#cb125-22" aria-hidden="true" tabindex="-1"></a>                 <span class="at">modelprior=</span><span class="fu">uniform</span>())</span>
<span id="cb125-23"><a href="stochastic-explorations-using-mcmc.html#cb125-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-24"><a href="stochastic-explorations-using-mcmc.html#cb125-24" aria-hidden="true" tabindex="-1"></a><span class="co"># AIC</span></span>
<span id="cb125-25"><a href="stochastic-explorations-using-mcmc.html#cb125-25" aria-hidden="true" tabindex="-1"></a>cog.AIC <span class="ot">=</span> <span class="fu">bas.lm</span>(kid_score <span class="sc">~</span> ., <span class="at">data=</span>cognitive, <span class="at">prior=</span><span class="st">&quot;AIC&quot;</span>, </span>
<span id="cb125-26"><a href="stochastic-explorations-using-mcmc.html#cb125-26" aria-hidden="true" tabindex="-1"></a>                 <span class="at">modelprior=</span><span class="fu">uniform</span>())</span></code></pre></div>
<p>Here <code>cog.g</code> is the model corresponding to the unit information prior <span class="math inline">\(g=n\)</span>. <code>cog.ZS</code> is the model under the Zellner-Siow cauchy prior with Jeffrey’s reference prior on <span class="math inline">\(\sigma^2\)</span>. <code>cog.HG</code> gives the model under the hyper-<span class="math inline">\(g/n\)</span> prior. <code>cog.EB</code> is the empirical Bayesian estimates which maximizes the marginal likelihood. <code>cog.BIC</code> and <code>cog.AIC</code> are the ones corresponding to using <code>BIC</code> and <code>AIC</code> for marginal likelihood approximation.</p>
<p>In order to compare the posterior inclusion probability (pip) of each coefficient, we group the results <span class="math inline">\(p(\beta_i\neq 0)\)</span> obtained from the <code>probne0</code> attribute of each model for later comparison</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="stochastic-explorations-using-mcmc.html#cb126-1" aria-hidden="true" tabindex="-1"></a>probne0 <span class="ot">=</span> <span class="fu">cbind</span>(cog.BIC<span class="sc">$</span>probne0, cog.g<span class="sc">$</span>probne0, cog.ZS<span class="sc">$</span>probne0, cog.HG<span class="sc">$</span>probne0,</span>
<span id="cb126-2"><a href="stochastic-explorations-using-mcmc.html#cb126-2" aria-hidden="true" tabindex="-1"></a>                cog.EB<span class="sc">$</span>probne0, cog.AIC<span class="sc">$</span>probne0)</span>
<span id="cb126-3"><a href="stochastic-explorations-using-mcmc.html#cb126-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-4"><a href="stochastic-explorations-using-mcmc.html#cb126-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(probne0) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;BIC&quot;</span>, <span class="st">&quot;g&quot;</span>, <span class="st">&quot;ZS&quot;</span>, <span class="st">&quot;HG&quot;</span>, <span class="st">&quot;EB&quot;</span>, <span class="st">&quot;AIC&quot;</span>)</span>
<span id="cb126-5"><a href="stochastic-explorations-using-mcmc.html#cb126-5" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(probne0) <span class="ot">=</span> <span class="fu">c</span>(cog.BIC<span class="sc">$</span>namesx)</span></code></pre></div>
<p>We can compare the results by printing the matrix <code>probne0</code> that we just generated. If we want to visualize them to get a clearer idea, we may plot them using bar plots.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="stochastic-explorations-using-mcmc.html#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb127-2"><a href="stochastic-explorations-using-mcmc.html#cb127-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-3"><a href="stochastic-explorations-using-mcmc.html#cb127-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate plot for each variable and save in a list</span></span>
<span id="cb127-4"><a href="stochastic-explorations-using-mcmc.html#cb127-4" aria-hidden="true" tabindex="-1"></a>P <span class="ot">=</span> <span class="fu">list</span>()</span>
<span id="cb127-5"><a href="stochastic-explorations-using-mcmc.html#cb127-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>){</span>
<span id="cb127-6"><a href="stochastic-explorations-using-mcmc.html#cb127-6" aria-hidden="true" tabindex="-1"></a>  mydata <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">prior =</span> <span class="fu">colnames</span>(probne0), <span class="at">posterior =</span> probne0[i, ])</span>
<span id="cb127-7"><a href="stochastic-explorations-using-mcmc.html#cb127-7" aria-hidden="true" tabindex="-1"></a>  mydata<span class="sc">$</span>prior <span class="ot">=</span> <span class="fu">factor</span>(mydata<span class="sc">$</span>prior, <span class="at">levels =</span> <span class="fu">colnames</span>(probne0))</span>
<span id="cb127-8"><a href="stochastic-explorations-using-mcmc.html#cb127-8" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ggplot</span>(mydata, <span class="fu">aes</span>(<span class="at">x =</span> prior, <span class="at">y =</span> posterior)) <span class="sc">+</span></span>
<span id="cb127-9"><a href="stochastic-explorations-using-mcmc.html#cb127-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb127-10"><a href="stochastic-explorations-using-mcmc.html#cb127-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ylab</span>(<span class="st">&quot;&quot;</span>) <span class="sc">+</span> </span>
<span id="cb127-11"><a href="stochastic-explorations-using-mcmc.html#cb127-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ggtitle</span>(cog.g<span class="sc">$</span>namesx[i])</span>
<span id="cb127-12"><a href="stochastic-explorations-using-mcmc.html#cb127-12" aria-hidden="true" tabindex="-1"></a>  P <span class="ot">=</span> <span class="fu">c</span>(P, <span class="fu">list</span>(p))</span>
<span id="cb127-13"><a href="stochastic-explorations-using-mcmc.html#cb127-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb127-14"><a href="stochastic-explorations-using-mcmc.html#cb127-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-15"><a href="stochastic-explorations-using-mcmc.html#cb127-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cowplot)</span>
<span id="cb127-16"><a href="stochastic-explorations-using-mcmc.html#cb127-16" aria-hidden="true" tabindex="-1"></a><span class="fu">do.call</span>(plot_grid, <span class="fu">c</span>(P))</span></code></pre></div>
<p><img src="08-MCMC-02-prior_files/figure-html/plots-1.png" width="672" />
In the plots above, the <span class="math inline">\(x\)</span>-axis lists all the prior distributions we consider, and the bar heights represent the posterior inclusion probability of each coefficient, i.e., <span class="math inline">\(p(\beta_i\neq 0)\)</span>.</p>
<p>We can see that mother’s IQ score is included almost as probability 1 in all priors. So all methods agree that we should include variable <code>IQ</code>. Mother’s high school status also has probability of more than 0.5 in each prior, suggesting that we may also consider including the variable <code>hs</code>. However, mother’s work status and mother’s age have much lower posterior inclusion probability in all priors. From left to right in each bar plot, we see that method <code>BIC</code> is the most conservative method (meaning it will exclude the most variables), while <code>AIC</code> is being the less conservative method.</p>

</div>
</div>
<div id="r-demo-on-bas-package" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> R Demo on <code>BAS</code> Package<a href="stochastic-explorations-using-mcmc.html#r-demo-on-bas-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we will apply Bayesian model selection and model averaging on the US crime data set <code>UScrime</code> using the <code>BAS</code> package. We will introduce some additional diagnostic plots, and talk about the effect of multicollinearity in model uncertainty.</p>
<div id="the-uscrime-data-set-and-data-processing" class="section level3 hasAnchor" number="8.3.1">
<h3><span class="header-section-number">8.3.1</span> The <code>UScrime</code> Data Set and Data Processing<a href="stochastic-explorations-using-mcmc.html#the-uscrime-data-set-and-data-processing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will demo the <code>BAS</code> commands using the US crime data set in the R libarry <code>MASS</code>.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="stochastic-explorations-using-mcmc.html#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library and data set</span></span>
<span id="cb128-2"><a href="stochastic-explorations-using-mcmc.html#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb128-3"><a href="stochastic-explorations-using-mcmc.html#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(UScrime)</span></code></pre></div>
<p>This data set contains data on 47 states of the US for the year of 1960. The response variable <span class="math inline">\(Y\)</span> is the rate of crimes in a particular category per head of population of each state. There are 15 potential explanatory variables with values for each of the 47 states related to crime and other demographics. Here is the table of all the potential explanatory variables and their descriptions.</p>
<table>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>M</code></td>
<td>Percentage of males aged 14-24</td>
</tr>
<tr class="even">
<td><code>So</code></td>
<td>Indicator variable for southern states</td>
</tr>
<tr class="odd">
<td><code>Ed</code></td>
<td>Mean years of schooling</td>
</tr>
<tr class="even">
<td><code>Po1</code></td>
<td>Police expenditure in 1960</td>
</tr>
<tr class="odd">
<td><code>Po2</code></td>
<td>Police expenditure in 1959</td>
</tr>
<tr class="even">
<td><code>LF</code></td>
<td>Labour force participation rate</td>
</tr>
<tr class="odd">
<td><code>M.F</code></td>
<td>Number of males per 1000 females</td>
</tr>
<tr class="even">
<td><code>Pop</code></td>
<td>State population</td>
</tr>
<tr class="odd">
<td><code>NW</code></td>
<td>Number of non-whites per 1000 people</td>
</tr>
<tr class="even">
<td><code>U1</code></td>
<td>Unemployment rate of urban males aged 14-24</td>
</tr>
<tr class="odd">
<td><code>U2</code></td>
<td>Unemployment rate of urban males aged 35-39</td>
</tr>
<tr class="even">
<td><code>GDP</code></td>
<td>Gross domestic product per head</td>
</tr>
<tr class="odd">
<td><code>Ineq</code></td>
<td>Income inequality</td>
</tr>
<tr class="even">
<td><code>Prob</code></td>
<td>Probability of imprisonment</td>
</tr>
<tr class="odd">
<td><code>Time</code></td>
<td>Average time served in state prisons</td>
</tr>
</tbody>
</table>
<p><br/></p>
<p>We may use the <code>summary</code> function to describe each variable in the data set.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="stochastic-explorations-using-mcmc.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(UScrime)</span></code></pre></div>
<pre><code>##        M               So               Ed             Po1       
##  Min.   :119.0   Min.   :0.0000   Min.   : 87.0   Min.   : 45.0  
##  1st Qu.:130.0   1st Qu.:0.0000   1st Qu.: 97.5   1st Qu.: 62.5  
##  Median :136.0   Median :0.0000   Median :108.0   Median : 78.0  
##  Mean   :138.6   Mean   :0.3404   Mean   :105.6   Mean   : 85.0  
##  3rd Qu.:146.0   3rd Qu.:1.0000   3rd Qu.:114.5   3rd Qu.:104.5  
##  Max.   :177.0   Max.   :1.0000   Max.   :122.0   Max.   :166.0  
##       Po2               LF             M.F              Pop        
##  Min.   : 41.00   Min.   :480.0   Min.   : 934.0   Min.   :  3.00  
##  1st Qu.: 58.50   1st Qu.:530.5   1st Qu.: 964.5   1st Qu.: 10.00  
##  Median : 73.00   Median :560.0   Median : 977.0   Median : 25.00  
##  Mean   : 80.23   Mean   :561.2   Mean   : 983.0   Mean   : 36.62  
##  3rd Qu.: 97.00   3rd Qu.:593.0   3rd Qu.: 992.0   3rd Qu.: 41.50  
##  Max.   :157.00   Max.   :641.0   Max.   :1071.0   Max.   :168.00  
##        NW              U1               U2             GDP       
##  Min.   :  2.0   Min.   : 70.00   Min.   :20.00   Min.   :288.0  
##  1st Qu.: 24.0   1st Qu.: 80.50   1st Qu.:27.50   1st Qu.:459.5  
##  Median : 76.0   Median : 92.00   Median :34.00   Median :537.0  
##  Mean   :101.1   Mean   : 95.47   Mean   :33.98   Mean   :525.4  
##  3rd Qu.:132.5   3rd Qu.:104.00   3rd Qu.:38.50   3rd Qu.:591.5  
##  Max.   :423.0   Max.   :142.00   Max.   :58.00   Max.   :689.0  
##       Ineq            Prob              Time             y         
##  Min.   :126.0   Min.   :0.00690   Min.   :12.20   Min.   : 342.0  
##  1st Qu.:165.5   1st Qu.:0.03270   1st Qu.:21.60   1st Qu.: 658.5  
##  Median :176.0   Median :0.04210   Median :25.80   Median : 831.0  
##  Mean   :194.0   Mean   :0.04709   Mean   :26.60   Mean   : 905.1  
##  3rd Qu.:227.5   3rd Qu.:0.05445   3rd Qu.:30.45   3rd Qu.:1057.5  
##  Max.   :276.0   Max.   :0.11980   Max.   :44.00   Max.   :1993.0</code></pre>
<p>However, these variables have been pre-processed for modeling purpose, so the summary statistics may not be so meaningful. The values of all these variables have been aggregated over each state, so this is a case of ecological regression. We will not model directly the rate for a person to commit a crime. Instead, we will use the total number of crimes and average values of predictors at the state level to predict the total crime rate of each state.</p>
<p>We transform the variables using the natural log function, except the indicator variable <code>So</code> (2nd column of the data set). We perform this transformation based on the analysis of this data set.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> Notice that <code>So</code> is already a numeric variable (<code>1</code> indicating Southern state and <code>0</code> otherwise), not as a categorical variable. Hence we do not need any data processing of this variable, unlike mother’s high school status <code>hs</code> and mother’s work status <code>work</code> in the kid’s cognitive score data set.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="stochastic-explorations-using-mcmc.html#cb131-1" aria-hidden="true" tabindex="-1"></a>UScrime[,<span class="sc">-</span><span class="dv">2</span>] <span class="ot">=</span> <span class="fu">log</span>(UScrime[,<span class="sc">-</span><span class="dv">2</span>])</span></code></pre></div>
</div>
<div id="bayesian-models-and-diagnostics" class="section level3 hasAnchor" number="8.3.2">
<h3><span class="header-section-number">8.3.2</span> Bayesian Models and Diagnostics<a href="stochastic-explorations-using-mcmc.html#bayesian-models-and-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We run <code>bas.lm</code> function from the <code>BAS</code> package. We first run the full model and use this information for later decision on what variables to include. Here we have 15 potential predictors. The total number of models is<span class="math inline">\(\ 2^{15} = 32768\)</span>. This is not a very large number and <code>BAS</code> can enumerate all the models pretty quickly. However, we want to illustrate how to explore models using stochastic methods. Hence we set argument <code>method = MCMC</code> inside the <code>bas.lm</code> function. We also use the Zellner-Siow cauchy prior for the prior distributions of the coefficients in this regression.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="stochastic-explorations-using-mcmc.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BAS)</span>
<span id="cb132-2"><a href="stochastic-explorations-using-mcmc.html#cb132-2" aria-hidden="true" tabindex="-1"></a>crime.ZS <span class="ot">=</span>  <span class="fu">bas.lm</span>(y <span class="sc">~</span> ., <span class="at">data=</span>UScrime,</span>
<span id="cb132-3"><a href="stochastic-explorations-using-mcmc.html#cb132-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">prior=</span><span class="st">&quot;ZS-null&quot;</span>, <span class="at">modelprior=</span><span class="fu">uniform</span>(), <span class="at">method =</span> <span class="st">&quot;MCMC&quot;</span>) </span></code></pre></div>
<p><code>BAS</code> will run the MCMC sampler until the number of unique models in the sample exceeds <span class="math inline">\(\text{number of models} = 2^{p}\)</span> (when <span class="math inline">\(p &lt; 19\)</span>) or until the number of MCMC iterations exceeds <span class="math inline">\(2\times\text{number of models}\)</span> by default, whichever is smaller. Here <span class="math inline">\(p\)</span> is the number of predictors.</p>
<p><strong>Diagnostic Plots</strong></p>
<p>To analyze the result, we first look at the diagnostic plot using <code>diagnostics</code> function and see whether we have run the MCMC exploration long enough so that the posterior inclusion probability (pip) has converged.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="stochastic-explorations-using-mcmc.html#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">diagnostics</span>(crime.ZS, <span class="at">type=</span><span class="st">&quot;pip&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="08-MCMC-03-demo_files/figure-html/diagnostics-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>In this plot, the <span class="math inline">\(x\)</span>-axis is the renormalized posterior inclusion probability (pip) of each coefficient <span class="math inline">\(\beta_i,\ i=1,\cdots, 15\)</span> in this model. This can be calculated as
<span class="math display" id="eq:pip">\[\begin{equation}
p(\beta_i\neq 0~|~\text{data}) = \sum_{M_m\in\text{ model space}}I(X_i\in M_m)\left(\frac{\textit{BF}[M_m:M_0]\textit{O}[M_m:M_0]}{\displaystyle \sum_{M_j}\textit{BF}[M_j:M_0]\textit{O}[M_j:M_0]}\right).
\tag{8.2}
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(X_i\)</span> is the <span class="math inline">\(i\)</span>th predictor variable, and <span class="math inline">\(I(X_i\in M_m)\)</span> is the indicator function which is 1 if <span class="math inline">\(X_i\)</span> is included in model <span class="math inline">\(M_m\)</span> and 0 if <span class="math inline">\(X_i\)</span> is not included. The first <span class="math inline">\(\Sigma\)</span> notation indicates that we sum over all models <span class="math inline">\(M_m\)</span> in the model space. And we use
<span class="math display" id="eq:weights">\[\begin{equation}
\frac{\textit{BF}[M_m:M_0]\textit{O}[M_m:M_0]}{\displaystyle \sum_{M_j}\textit{BF}[M_j:M_0]\textit{O}[M_j:M_0]}
\tag{8.3}
\end{equation}\]</span>
as the weights. You may recognize that the numerator of <a href="stochastic-explorations-using-mcmc.html#eq:weights">(8.3)</a> is exactly the ratio of the posterior probability of model <span class="math inline">\(M_m\)</span> over the posterior probability of the null model <span class="math inline">\(M_0\)</span>, i.e., the posterier odd <span class="math inline">\(\textit{PO}[M_m:M_0]\)</span>. We devide the posterior odd by the total sum of posterior odds of all models in the model space, to make sure these weights are between 0 and 1. The weight in Equation <a href="stochastic-explorations-using-mcmc.html#eq:weights">(8.3)</a> represents the posterior probability of the model <span class="math inline">\(M_m\)</span> after seeing the data <span class="math inline">\(p(M_m~|~\text{data})\)</span>, the one we used in Section <a href="bayesian-model-choice.html#sec:BMU">7.2</a>. So Equation <a href="stochastic-explorations-using-mcmc.html#eq:pip">(8.2)</a> is the theoretical calculation of pip, which can be rewrited as
<span class="math display">\[ p(\beta_i\neq 0~|~\text{data}) = \sum_{M_m\in \text{ model space}}I(X_i\in M_m)p(M_m~|~\text{data}). \]</span>
The null model <span class="math inline">\(M_0\)</span>, as we recall, is the model that only includes the intercept.</p>
<p>On the <span class="math inline">\(y\)</span>-axis of the plot, we lay out the posterior inclusion probability of coefficient <span class="math inline">\(\beta_i\)</span>, which is calculated using
<span class="math display">\[ p(\beta_i\neq 0~|~\text{data}) = \frac{1}{J}\sum_{j=1}^J I(X_i\in M^{(j)}).\]</span>
Here <span class="math inline">\(J\)</span> is the total number of models that we sample using MCMC; each model is denoted as <span class="math inline">\(M^{(j)}\)</span> (some models may repeat themselves in the sample). We count the frequency of variable <span class="math inline">\(X_i\)</span> occuring in model <span class="math inline">\(M^{(j)}\)</span>, and divide this number by the total number of models <span class="math inline">\(J\)</span>. This is a frequentist approach to approximate the posterior probability of including <span class="math inline">\(X_i\)</span> after seeing the data.</p>
<p>When all points are on the 45 degree diagonal, we say that the posterior inclusion probability of each variable from MCMC have converged well enough to the theoretical posterior inclusion probability.</p>
<p>We can also use <code>diagnostics</code> function to see whether the model posterior probability has converged:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="stochastic-explorations-using-mcmc.html#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">diagnostics</span>(crime.ZS, <span class="at">type =</span> <span class="st">&quot;model&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="08-MCMC-03-demo_files/figure-html/model-prob-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can see that some of the points still fall slightly away from the 45 degree diagonal line. This may suggest we should increase the number of MCMC iterations. We may do that by imposing the argument on <code>MCMC.iterations</code> inside the <code>bas.lm</code> function</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="stochastic-explorations-using-mcmc.html#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-run regression using larger number of MCMC iterations</span></span>
<span id="cb135-2"><a href="stochastic-explorations-using-mcmc.html#cb135-2" aria-hidden="true" tabindex="-1"></a>crime.ZS <span class="ot">=</span> <span class="fu">bas.lm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> UScrime,</span>
<span id="cb135-3"><a href="stochastic-explorations-using-mcmc.html#cb135-3" aria-hidden="true" tabindex="-1"></a>                  <span class="at">prior =</span> <span class="st">&quot;ZS-null&quot;</span>, <span class="at">modelprior =</span> <span class="fu">uniform</span>(),</span>
<span id="cb135-4"><a href="stochastic-explorations-using-mcmc.html#cb135-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">method =</span> <span class="st">&quot;MCMC&quot;</span>, <span class="at">MCMC.iterations =</span> <span class="dv">10</span> <span class="sc">^</span> <span class="dv">6</span>)</span>
<span id="cb135-5"><a href="stochastic-explorations-using-mcmc.html#cb135-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-6"><a href="stochastic-explorations-using-mcmc.html#cb135-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot diagnostics again</span></span>
<span id="cb135-7"><a href="stochastic-explorations-using-mcmc.html#cb135-7" aria-hidden="true" tabindex="-1"></a><span class="fu">diagnostics</span>(crime.ZS, <span class="at">type =</span> <span class="st">&quot;model&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="08-MCMC-03-demo_files/figure-html/more-MCMC-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>With more number of iterations, we see that most points stay in the 45 degree diagonal line, meaing the posterior inclusion probability from the MCMC method has mostly converged to the theoretical posterior inclusion probability.</p>
<p>We will next look at four other plots of the <code>BAS</code> object, <code>crime.ZS</code>.</p>
<p><strong>Residuals Versus Fitted Values Using BMA</strong></p>
<p>The first plot is the residuals over the fitted value under Bayesian model averaging results.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="stochastic-explorations-using-mcmc.html#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(crime.ZS, <span class="at">which =</span> <span class="dv">1</span>, <span class="at">add.smooth =</span> F, </span>
<span id="cb136-2"><a href="stochastic-explorations-using-mcmc.html#cb136-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ask =</span> F, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">sub.caption=</span><span class="st">&quot;&quot;</span>, <span class="at">caption=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb136-3"><a href="stochastic-explorations-using-mcmc.html#cb136-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="08-MCMC-03-demo_files/figure-html/plot1-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can see that the residuals lie around the dash line <span class="math inline">\(y=0\)</span>, and has a constant variance. Observations 11, 22, and 46 may be the potential outliers, which are indicated in the plot.</p>
<p><strong>Cumulative Sampled Probability</strong></p>
<p>The second plot shows the cumulative sampled model probability.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="stochastic-explorations-using-mcmc.html#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(crime.ZS, <span class="at">which=</span><span class="dv">2</span>, <span class="at">add.smooth =</span> F, <span class="at">sub.caption=</span><span class="st">&quot;&quot;</span>, <span class="at">caption=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="08-MCMC-03-demo_files/figure-html/CMP-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can see that after we have discovered about 5,000 unique models with MCMC sampling, the probability is starting to level off, indicating that these additional models have very small probability and do not contribute substantially to the posterior distribution. These probabilities are proportional to the product of marginal likelihoods of models and priors, <span class="math inline">\(p(\text{data}~|~M_m)p(M_m)\)</span>, rather than Monte Carlo frequencies.</p>
<p><strong>Model Complexity</strong></p>
<p>The third plot is the model size versus the natural log of the marginal likelihood, or the Bayes factor, to compare each model to the null model.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="stochastic-explorations-using-mcmc.html#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(crime.ZS, <span class="at">which=</span><span class="dv">3</span>, <span class="at">ask=</span>F, <span class="at">caption=</span><span class="st">&quot;&quot;</span>, <span class="at">sub.caption=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="08-MCMC-03-demo_files/figure-html/model-comp-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see that the models with the highest Bayes factors or logs of marginal likelihoods have around 8 or 9 predictors. The null model has a log of marginal likelihood of 0, or a Bayes factor of 1.</p>
<p><strong>Marginal Inclusion Probability</strong></p>
<p>Finally, we have a plot showing the importance of different predictors.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="stochastic-explorations-using-mcmc.html#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(crime.ZS, <span class="at">which =</span> <span class="dv">4</span>, <span class="at">ask =</span> F, <span class="at">caption =</span> <span class="st">&quot;&quot;</span>, <span class="at">sub.caption =</span> <span class="st">&quot;&quot;</span>, </span>
<span id="cb139-2"><a href="stochastic-explorations-using-mcmc.html#cb139-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">col.in =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">col.ex =</span> <span class="st">&quot;darkgrey&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="08-MCMC-03-demo_files/figure-html/MIP-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The lines in blue correspond to the variables where the marginal posterior inclusion probability (pip), is greater than 0.5, suggesting that these variables are important for prediction. The variables represented in grey lines have posterior inclusion probability less than 0.5. Small posterior inclusion probability may arise when two or more variables are highly correlated, similar to large <span class="math inline">\(p\)</span>-values with multicollinearity. So we should be cautious to use these posterior inclusion probabilities to eliminate variables.</p>
<p><strong>Model Space Visualization</strong></p>
<p>To focus on the high posterior probability models, we can look at the image of the model space.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="stochastic-explorations-using-mcmc.html#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(crime.ZS, <span class="at">rotate =</span> F)</span></code></pre></div>
<pre><code>## Warning in par(par.old): argument 1 does not name a graphical parameter</code></pre>
<p><img src="08-MCMC-03-demo_files/figure-html/model-space-1.png" width="672" /></p>
<p>By default, we only include the top 20 models. An interesting feature of this plot is, that whenever <code>Po1</code>, the police expenditures in 1960, is included, <code>Po2</code>, the police expenditures in 1959, will be excluded from the model, and vice versa.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="stochastic-explorations-using-mcmc.html#cb142-1" aria-hidden="true" tabindex="-1"></a>out <span class="ot">=</span> <span class="fu">cor</span>(UScrime<span class="sc">$</span>Po1, UScrime<span class="sc">$</span>Po2)</span>
<span id="cb142-2"><a href="stochastic-explorations-using-mcmc.html#cb142-2" aria-hidden="true" tabindex="-1"></a>out </span></code></pre></div>
<pre><code>## [1] 0.9933688</code></pre>
<p>Calculating the correlation between the two variables, we see that that <code>Po1</code> and <code>Po2</code> are highly correlated with positive correlation 0.993.</p>
</div>
<div id="posterior-uncertainty-in-coefficients" class="section level3 hasAnchor" number="8.3.3">
<h3><span class="header-section-number">8.3.3</span> Posterior Uncertainty in Coefficients<a href="stochastic-explorations-using-mcmc.html#posterior-uncertainty-in-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Due to the interesting inclusion relationship between <code>Po1</code> and <code>Po2</code> in the top 20 models, we extract the two coefficients under Bayesian model averaging and take a look at the plots for the coefficients for <code>Po1</code> and <code>Po2</code>.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="stochastic-explorations-using-mcmc.html#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract coefficients</span></span>
<span id="cb144-2"><a href="stochastic-explorations-using-mcmc.html#cb144-2" aria-hidden="true" tabindex="-1"></a>coef.ZS<span class="ot">=</span><span class="fu">coef</span>(crime.ZS)</span>
<span id="cb144-3"><a href="stochastic-explorations-using-mcmc.html#cb144-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-4"><a href="stochastic-explorations-using-mcmc.html#cb144-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Po1 and Po2 are in the 5th and 6th columns in UScrime</span></span>
<span id="cb144-5"><a href="stochastic-explorations-using-mcmc.html#cb144-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb144-6"><a href="stochastic-explorations-using-mcmc.html#cb144-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(coef.ZS, <span class="at">subset =</span> <span class="fu">c</span>(<span class="dv">5</span><span class="sc">:</span><span class="dv">6</span>), </span>
<span id="cb144-7"><a href="stochastic-explorations-using-mcmc.html#cb144-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">col.lab =</span> <span class="st">&quot;darkgrey&quot;</span>, <span class="at">col.axis =</span> <span class="st">&quot;darkgrey&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>, <span class="at">ask =</span> F)</span></code></pre></div>
<p><img src="08-MCMC-03-demo_files/figure-html/Po1-Po2-plots-1.png" width="672" /></p>
<p>Under Bayesian model averaging, there is more mass at 0 for <code>Po2</code> than <code>Po1</code>, giving more posterior inclusion probability for <code>Po1</code>. This is also the reason why in the marginal posterior plot of variable importance, <code>Po1</code> has a blue line while <code>Po2</code> has a grey line. When <code>Po1</code> is excluded, the distributions of other coefficients in the model, except the one for <code>Po2</code>, will have similar distributions as when both <code>Po1</code> and <code>Po2</code> are in the model. However, when both predictors are included, the adjusted coefficient for <code>Po2</code> has more support on negative values, since we are over compensating for having both variables included in the model. In extreme cases of correlations, one may find that the coefficient plot is multimodal. If this is the case, the posterior mean may not be in the highest probability density credible interval, and this mean is not necessarily an informative summary. We will discuss more in the next section about making decisions on highly correlated variables.</p>
<p>We can read the credible intervals of each variable using the <code>confint</code> function on the coefficient object <code>coef.ZS</code> of the model. Here we round the results in 4 decimal places.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="stochastic-explorations-using-mcmc.html#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">confint</span>(coef.ZS), <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##              2.5%  97.5%    beta
## Intercept  6.6637 6.7779  6.7249
## M          0.0000 2.1646  1.1429
## So        -0.0609 0.3032  0.0354
## Ed         0.6589 3.1973  1.8595
## Po1        0.0000 1.4578  0.6049
## Po2       -0.3144 1.3778  0.3151
## LF        -0.5144 1.0117  0.0583
## M.F       -2.3180 1.9704 -0.0231
## Pop       -0.1268 0.0065 -0.0222
## NW         0.0000 0.1663  0.0664
## U1        -0.5294 0.3467 -0.0244
## U2        -0.0019 0.6492  0.2067
## GDP       -0.0611 1.1522  0.2042
## Ineq       0.7028 2.1345  1.3903
## Prob      -0.4115 0.0000 -0.2150
## Time      -0.5194 0.0504 -0.0837
## attr(,&quot;Probability&quot;)
## [1] 0.95
## attr(,&quot;class&quot;)
## [1] &quot;confint.bas&quot;</code></pre>
</div>
<div id="prediction" class="section level3 hasAnchor" number="8.3.4">
<h3><span class="header-section-number">8.3.4</span> Prediction<a href="stochastic-explorations-using-mcmc.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use the usual <code>predict</code> function that we used for <code>lm</code> objects to obtain prediction from the <code>BAS</code> object <code>crime.ZS</code>. However, since we have different models to choose from under the Bayesian framework, we need to first specify which particular model we use to obtain the prediction. For example, if we would like to use the Bayesian model averaging results for coefficients to obtain predictions, we would specify the <code>estimator</code> argument in the <code>predict</code> function like the following</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="stochastic-explorations-using-mcmc.html#cb147-1" aria-hidden="true" tabindex="-1"></a>crime.BMA <span class="ot">=</span> <span class="fu">predict</span>(crime.ZS, <span class="at">estimator =</span> <span class="st">&quot;BMA&quot;</span>, <span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>The fitted values can be obtained using the <code>fit</code> attribute of <code>crime.BMA</code>. We have transposed the fitted values into a vector to better present all the values.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="stochastic-explorations-using-mcmc.html#cb148-1" aria-hidden="true" tabindex="-1"></a>fitted <span class="ot">=</span> crime.BMA<span class="sc">$</span>fit</span>
<span id="cb148-2"><a href="stochastic-explorations-using-mcmc.html#cb148-2" aria-hidden="true" tabindex="-1"></a><span class="fu">as.vector</span>(fitted)</span></code></pre></div>
<pre><code>##  [1] 6.661523 7.298633 6.179181 7.610918 7.054620 6.513706 6.783577 7.266631
##  [9] 6.629256 6.601253 7.054966 6.570552 6.473296 6.582135 6.556940 6.904770
## [17] 6.229518 6.809451 6.943239 6.962062 6.609524 6.429458 6.898664 6.776913
## [25] 6.406129 7.400904 6.019591 7.156416 7.090363 6.500313 6.208760 6.606196
## [33] 6.797755 6.820131 6.625819 7.028788 6.793341 6.363582 6.603191 7.045172
## [41] 6.548082 6.046189 6.929764 7.005957 6.235977 6.608952 6.829845</code></pre>
<p>We may use these fitted values for further error calculations. We will talk about decision making on models and how to obtain predictions under different models in the next section.</p>

</div>
</div>
<div id="decision-making-under-model-uncertainty" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Decision Making Under Model Uncertainty<a href="stochastic-explorations-using-mcmc.html#decision-making-under-model-uncertainty" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We are closing this chapter by presenting the last topic, decision making under model uncertainty. We have seen that under the Bayesian framework, we can use different prior distributions for coefficients, different model priors for models, and we can even use stochastic exploration methods for complex model selections. After selecting these coefficient priors and model priors, we can obtain the marginal posterior inclusion probability for each variable in the full model, which may provide some information about whether or not to include a particular variable in the model for further model analysis and predictions. With all the information presented in the results, which model would be the most appropriate model?</p>
<p>In this section, we will talk about different methods for selecting models and decision making for posterior distributions and predictions. We will illustrate this process using the US crime data <code>UScrime</code> as an example and process it using the <code>BAS</code> package.</p>
<p>We first prepare the data as in the last section and run <code>bas.lm</code> on the full model</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="stochastic-explorations-using-mcmc.html#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(UScrime, <span class="at">package=</span><span class="st">&quot;MASS&quot;</span>)</span>
<span id="cb150-2"><a href="stochastic-explorations-using-mcmc.html#cb150-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-3"><a href="stochastic-explorations-using-mcmc.html#cb150-3" aria-hidden="true" tabindex="-1"></a><span class="co"># take the natural log transform on the variables except the 2nd column `So`</span></span>
<span id="cb150-4"><a href="stochastic-explorations-using-mcmc.html#cb150-4" aria-hidden="true" tabindex="-1"></a>UScrime[, <span class="sc">-</span><span class="dv">2</span>] <span class="ot">=</span> <span class="fu">log</span>(UScrime[, <span class="sc">-</span><span class="dv">2</span>])</span>
<span id="cb150-5"><a href="stochastic-explorations-using-mcmc.html#cb150-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-6"><a href="stochastic-explorations-using-mcmc.html#cb150-6" aria-hidden="true" tabindex="-1"></a><span class="co"># run Bayesian linear regression</span></span>
<span id="cb150-7"><a href="stochastic-explorations-using-mcmc.html#cb150-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BAS)</span>
<span id="cb150-8"><a href="stochastic-explorations-using-mcmc.html#cb150-8" aria-hidden="true" tabindex="-1"></a>crime.ZS <span class="ot">=</span>  <span class="fu">bas.lm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> UScrime,</span>
<span id="cb150-9"><a href="stochastic-explorations-using-mcmc.html#cb150-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">prior =</span> <span class="st">&quot;ZS-null&quot;</span>, <span class="at">modelprior =</span> <span class="fu">uniform</span>()) </span></code></pre></div>
<div id="model-choice" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> Model Choice<a href="stochastic-explorations-using-mcmc.html#model-choice" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For Bayesian model choice, we start with the full model, which includes all the predictors. The uncertainty of selecting variables, or model uncertainty that we have been discussing, arises when we believe that some of the explanatory variables may be unrelated to the response variable. This corresponds to setting a regression coefficient <span class="math inline">\(\beta_j\)</span> to be exactly zero. We specify prior distributions that reflect our uncertainty about the importance of variables. We then update the model based on the data we obtained, resulting in posterior distributions over all models and the coefficients and variances within each model.</p>
<p>Now the question has become, how to select a single model from the posterior distribution and use it for furture inference? What are the objectives from inference?</p>
<p><strong>BMA Model</strong></p>
<p>We do have a single model, the one that is obtained by averaging all models using their posterior probabilities, the Bayesian model averaging model, or BMA. This is referred to as a hierarchical model and it is composed of many simpler models as building blocks. This represents the full posterior uncertainty after seeing the data.</p>
<p>We can obtain the posterior predictive mean by using the weighted average of all of the predictions from each sub model</p>
<p><span class="math display">\[\hat{\mu} = E[\hat{Y}~|~\text{data}] = \sum_{M_m \in \text{ model space}}\hat{Y}\times p(M_m~|~\text{data}).\]</span>
This prediction is the best under the squared error loss <span class="math inline">\(L_2\)</span>. From <code>BAS</code>, we can obtain predictions and fitted values using the usual <code>predict</code> and <code>fitted</code> functions. To specify which model we use for these results, we need to include argument <code>estimator</code>.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="stochastic-explorations-using-mcmc.html#cb151-1" aria-hidden="true" tabindex="-1"></a>crime.BMA <span class="ot">=</span> <span class="fu">predict</span>(crime.ZS, <span class="at">estimator =</span> <span class="st">&quot;BMA&quot;</span>)</span>
<span id="cb151-2"><a href="stochastic-explorations-using-mcmc.html#cb151-2" aria-hidden="true" tabindex="-1"></a>mu_hat <span class="ot">=</span> <span class="fu">fitted</span>(crime.ZS, <span class="at">estimator =</span> <span class="st">&quot;BMA&quot;</span>)</span></code></pre></div>
<p><code>crime.BMA</code>, the object obtained by the <code>predict</code> function, has additional slots storing results from the BMA model.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="stochastic-explorations-using-mcmc.html#cb152-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(crime.BMA)</span></code></pre></div>
<pre><code>##  [1] &quot;fit&quot;         &quot;Ybma&quot;        &quot;Ypred&quot;       &quot;postprobs&quot;   &quot;se.fit&quot;     
##  [6] &quot;se.pred&quot;     &quot;se.bma.fit&quot;  &quot;se.bma.pred&quot; &quot;df&quot;          &quot;best&quot;       
## [11] &quot;bestmodel&quot;   &quot;best.vars&quot;   &quot;estimator&quot;</code></pre>
<p>Plotting the two sets of fitted values, one obtained from the <code>fitted</code> function, another obtained from the <code>fit</code> attribute of the <code>predict</code> object <code>crime.BMA</code>, we see that they are in perfect agreement.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="stochastic-explorations-using-mcmc.html#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library and prepare data frame</span></span>
<span id="cb154-2"><a href="stochastic-explorations-using-mcmc.html#cb154-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb154-3"><a href="stochastic-explorations-using-mcmc.html#cb154-3" aria-hidden="true" tabindex="-1"></a>output <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">mu_hat =</span> mu_hat, <span class="at">fitted =</span> crime.BMA<span class="sc">$</span>fit)</span>
<span id="cb154-4"><a href="stochastic-explorations-using-mcmc.html#cb154-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-5"><a href="stochastic-explorations-using-mcmc.html#cb154-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot result from `fitted` function and result from `fit` attribute</span></span>
<span id="cb154-6"><a href="stochastic-explorations-using-mcmc.html#cb154-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> output, <span class="fu">aes</span>(<span class="at">x =</span> mu_hat, <span class="at">y =</span> fitted)) <span class="sc">+</span> </span>
<span id="cb154-7"><a href="stochastic-explorations-using-mcmc.html#cb154-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">pch =</span> <span class="dv">16</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb154-8"><a href="stochastic-explorations-using-mcmc.html#cb154-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb154-9"><a href="stochastic-explorations-using-mcmc.html#cb154-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(<span class="fu">hat</span>(mu[i]))) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="fu">expression</span>(<span class="fu">hat</span>(Y[i])))</span></code></pre></div>
<p><img src="08-MCMC-04-decisions_files/figure-html/BMA-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p><strong>Highest Probability Model</strong></p>
<p>If our objective is to learn what is the most likely model to have generated the data using a 0-1 loss <span class="math inline">\(L_0\)</span>, then the highest probability model (HPM) is optimal.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="stochastic-explorations-using-mcmc.html#cb155-1" aria-hidden="true" tabindex="-1"></a>crime.HPM <span class="ot">=</span> <span class="fu">predict</span>(crime.ZS, <span class="at">estimator =</span> <span class="st">&quot;HPM&quot;</span>)</span></code></pre></div>
<p>The variables selected from this model can be obtained using the <code>bestmodel</code> attribute from the <code>crime.HPM</code> object. We extract the variable names from the <code>crime.HPM</code></p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="stochastic-explorations-using-mcmc.html#cb156-1" aria-hidden="true" tabindex="-1"></a>crime.HPM<span class="sc">$</span>best.vars</span></code></pre></div>
<pre><code>## [1] &quot;Intercept&quot; &quot;M&quot;         &quot;Ed&quot;        &quot;Po1&quot;       &quot;NW&quot;        &quot;U2&quot;       
## [7] &quot;Ineq&quot;      &quot;Prob&quot;      &quot;Time&quot;</code></pre>
<p>We see that, except the intercept, which is always in any models, the highest probability model also includes <code>M</code>, percentage of males aged 14-24; <code>Ed</code>, mean years of schooling; <code>Po1</code>, police expenditures in 1960; <code>NW</code>, number of non-whites per 1000 people; <code>U2</code>, unemployment rate of urban males aged 35-39; <code>Ineq</code>, income inequlity; <code>Prob</code>, probability of imprisonment, and <code>Time</code>, average time in state prison.</p>
<p>To obtain the coefficients and their posterior means and posterior standard deviations, we tell give an optional argument to <code>coef</code> to indicate that we want to extract coefficients under the HPM.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="stochastic-explorations-using-mcmc.html#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select coefficients of HPM</span></span>
<span id="cb158-2"><a href="stochastic-explorations-using-mcmc.html#cb158-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb158-3"><a href="stochastic-explorations-using-mcmc.html#cb158-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Posterior means of coefficients</span></span>
<span id="cb158-4"><a href="stochastic-explorations-using-mcmc.html#cb158-4" aria-hidden="true" tabindex="-1"></a>coef.crime.ZS <span class="ot">=</span> <span class="fu">coef</span>(crime.ZS, <span class="at">estimator=</span><span class="st">&quot;HPM&quot;</span>)</span>
<span id="cb158-5"><a href="stochastic-explorations-using-mcmc.html#cb158-5" aria-hidden="true" tabindex="-1"></a>coef.crime.ZS</span></code></pre></div>
<pre><code>## 
##  Marginal Posterior Summaries of Coefficients: 
## 
##  Using  HPM 
## 
##  Based on the top  1 models 
##            post mean  post SD   post p(B != 0)
## Intercept   6.72494    0.02623   1.00000      
## M           1.42422    0.42278   0.85357      
## So          0.00000    0.00000   0.27371      
## Ed          2.14031    0.43094   0.97466      
## Po1         0.82141    0.15927   0.66516      
## Po2         0.00000    0.00000   0.44901      
## LF          0.00000    0.00000   0.20224      
## M.F         0.00000    0.00000   0.20497      
## Pop         0.00000    0.00000   0.36961      
## NW          0.10491    0.03821   0.69441      
## U1          0.00000    0.00000   0.25258      
## U2          0.27823    0.12492   0.61494      
## GDP         0.00000    0.00000   0.36012      
## Ineq        1.19269    0.27734   0.99654      
## Prob       -0.29910    0.08724   0.89918      
## Time       -0.27616    0.14574   0.37180</code></pre>
<p>We can also obtain the posterior probability of this model using</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="stochastic-explorations-using-mcmc.html#cb160-1" aria-hidden="true" tabindex="-1"></a>postprob.HPM <span class="ot">=</span> crime.ZS<span class="sc">$</span>postprobs[crime.HPM<span class="sc">$</span>best]</span>
<span id="cb160-2"><a href="stochastic-explorations-using-mcmc.html#cb160-2" aria-hidden="true" tabindex="-1"></a>postprob.HPM</span></code></pre></div>
<pre><code>## [1] 0.01824728</code></pre>
<p>we see that this highest probability model has posterior probability of only 0.018. There are many models that have comparable posterior probabilities. So even this model has the highest posterior probability, we are still pretty unsure about whether it is the best model.</p>
<p><strong>Median Probability Model</strong></p>
<p>Another model that is frequently reported, is the median probability model (MPM). This model includes all predictors whose marginal posterior inclusion probabilities are greater than 0.5. If the variables are all uncorrelated, this will be the same as the highest posterior probability model. For a sequence of nested models such as polynomial regression with increasing powers, the median probability model is the best single model for prediction.</p>
<p>However, since in the US crime example, <code>Po1</code> and <code>Po2</code> are highly correlated, we see that the variables included in MPM are slightly different than the variables included in HPM.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="stochastic-explorations-using-mcmc.html#cb162-1" aria-hidden="true" tabindex="-1"></a>crime.MPM <span class="ot">=</span> <span class="fu">predict</span>(crime.ZS, <span class="at">estimator =</span> <span class="st">&quot;MPM&quot;</span>)</span>
<span id="cb162-2"><a href="stochastic-explorations-using-mcmc.html#cb162-2" aria-hidden="true" tabindex="-1"></a>crime.MPM<span class="sc">$</span>best.vars</span></code></pre></div>
<pre><code>## [1] &quot;Intercept&quot; &quot;M&quot;         &quot;Ed&quot;        &quot;Po1&quot;       &quot;NW&quot;        &quot;U2&quot;       
## [7] &quot;Ineq&quot;      &quot;Prob&quot;</code></pre>
<p>As we see, this model only includes 7 variables, <code>M</code>, <code>Ed</code>, <code>Po1</code>, <code>NW</code>, <code>U2</code>, <code>Ineq</code>, and <code>Prob</code>. It does not include <code>Time</code> variable as in HPM.</p>
<p>When there are correlated predictors in non-nested models, MPM in general does well. However, if the correlations among variables increase, MPM may miss important variables as the correlations tend to dilute the posterior inclusing probabilities of related variables.</p>
<p>To obtain the coefficients in the median probability model, we specify that the estimator is now “MPM”:</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="stochastic-explorations-using-mcmc.html#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain coefficients of the  Median Probabilty Model</span></span>
<span id="cb164-2"><a href="stochastic-explorations-using-mcmc.html#cb164-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(crime.ZS, <span class="at">estimator =</span> <span class="st">&quot;MPM&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Marginal Posterior Summaries of Coefficients: 
## 
##  Using  MPM 
## 
##  Based on the top  1 models 
##            post mean  post SD   post p(B != 0)
## Intercept   6.72494    0.02713   1.00000      
## M           1.46180    0.43727   1.00000      
## So          0.00000    0.00000   0.00000      
## Ed          2.30642    0.43727   1.00000      
## Po1         0.87886    0.16204   1.00000      
## Po2         0.00000    0.00000   0.00000      
## LF          0.00000    0.00000   0.00000      
## M.F         0.00000    0.00000   0.00000      
## Pop         0.00000    0.00000   0.00000      
## NW          0.08162    0.03743   1.00000      
## U1          0.00000    0.00000   0.00000      
## U2          0.31053    0.12816   1.00000      
## GDP         0.00000    0.00000   0.00000      
## Ineq        1.18815    0.28710   1.00000      
## Prob       -0.18401    0.06466   1.00000      
## Time        0.00000    0.00000   0.00000</code></pre>
<p><strong>Best Predictive Model</strong></p>
<p>If our objective is prediction from a single model, the best choice is to find the model whose predictions are closest to those given by BMA. “Closest” could be based on squared error loss for predictions, or be based on any other loss functions. Unfortunately, there is no nice expression for this model. However, we can still calculate the loss for each of our sampled models to try to identify this best predictive model, or BPM.</p>
<p>Using the squared error loss, we find that the best predictive model is the one whose predictions are closest to BMA.</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="stochastic-explorations-using-mcmc.html#cb166-1" aria-hidden="true" tabindex="-1"></a>crime.BPM <span class="ot">=</span> <span class="fu">predict</span>(crime.ZS, <span class="at">estimator =</span> <span class="st">&quot;BPM&quot;</span>)</span>
<span id="cb166-2"><a href="stochastic-explorations-using-mcmc.html#cb166-2" aria-hidden="true" tabindex="-1"></a>crime.BPM<span class="sc">$</span>best.vars</span></code></pre></div>
<pre><code>##  [1] &quot;Intercept&quot; &quot;M&quot;         &quot;So&quot;        &quot;Ed&quot;        &quot;Po1&quot;       &quot;Po2&quot;      
##  [7] &quot;M.F&quot;       &quot;NW&quot;        &quot;U2&quot;        &quot;Ineq&quot;      &quot;Prob&quot;</code></pre>
<p>The best predictive model includes not only the 7 variables that MPM includes, but also <code>M.F</code>, number of males per 1000 females, and <code>Po2</code>, the police expenditures in 1959.</p>
<p>Using the <code>se.fit = TRUE</code> option with <code>predict</code> we can calculate standard deviations for the predictions or for the mean. Then we can use this as input for the <code>confint</code> function for the prediction object. Here we only show the results of the first 20 data points.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="stochastic-explorations-using-mcmc.html#cb168-1" aria-hidden="true" tabindex="-1"></a>crime.BPM <span class="ot">=</span> <span class="fu">predict</span>(crime.ZS, <span class="at">estimator =</span> <span class="st">&quot;BPM&quot;</span>, <span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb168-2"><a href="stochastic-explorations-using-mcmc.html#cb168-2" aria-hidden="true" tabindex="-1"></a>crime.BPM.conf.fit <span class="ot">=</span> <span class="fu">confint</span>(crime.BPM, <span class="at">parm =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb168-3"><a href="stochastic-explorations-using-mcmc.html#cb168-3" aria-hidden="true" tabindex="-1"></a>crime.BPM.conf.pred <span class="ot">=</span> <span class="fu">confint</span>(crime.BPM, <span class="at">parm =</span> <span class="st">&quot;pred&quot;</span>)</span>
<span id="cb168-4"><a href="stochastic-explorations-using-mcmc.html#cb168-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(crime.BPM<span class="sc">$</span>fit, crime.BPM.conf.fit, crime.BPM.conf.pred)</span>
<span id="cb168-5"><a href="stochastic-explorations-using-mcmc.html#cb168-5" aria-hidden="true" tabindex="-1"></a><span class="do">##                    2.5%    97.5%     mean     2.5%    97.5%     pred</span></span>
<span id="cb168-6"><a href="stochastic-explorations-using-mcmc.html#cb168-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1,] 6.668988 6.513238 6.824738 6.668988 6.258715 7.079261 6.668988</span></span>
<span id="cb168-7"><a href="stochastic-explorations-using-mcmc.html#cb168-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [2,] 7.290854 7.151787 7.429921 7.290854 6.886619 7.695089 7.290854</span></span>
<span id="cb168-8"><a href="stochastic-explorations-using-mcmc.html#cb168-8" aria-hidden="true" tabindex="-1"></a><span class="do">##  [3,] 6.202166 6.039978 6.364354 6.202166 5.789406 6.614926 6.202166</span></span>
<span id="cb168-9"><a href="stochastic-explorations-using-mcmc.html#cb168-9" aria-hidden="true" tabindex="-1"></a><span class="do">##  [4,] 7.661307 7.490608 7.832006 7.661307 7.245129 8.077484 7.661307</span></span>
<span id="cb168-10"><a href="stochastic-explorations-using-mcmc.html#cb168-10" aria-hidden="true" tabindex="-1"></a><span class="do">##  [5,] 7.015570 6.847647 7.183493 7.015570 6.600523 7.430617 7.015570</span></span>
<span id="cb168-11"><a href="stochastic-explorations-using-mcmc.html#cb168-11" aria-hidden="true" tabindex="-1"></a><span class="do">##  [6,] 6.469547 6.279276 6.659818 6.469547 6.044966 6.894128 6.469547</span></span>
<span id="cb168-12"><a href="stochastic-explorations-using-mcmc.html#cb168-12" aria-hidden="true" tabindex="-1"></a><span class="do">##  [7,] 6.776133 6.555130 6.997135 6.776133 6.336920 7.215346 6.776133</span></span>
<span id="cb168-13"><a href="stochastic-explorations-using-mcmc.html#cb168-13" aria-hidden="true" tabindex="-1"></a><span class="do">##  [8,] 7.299560 7.117166 7.481955 7.299560 6.878450 7.720670 7.299560</span></span>
<span id="cb168-14"><a href="stochastic-explorations-using-mcmc.html#cb168-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  [9,] 6.614927 6.482384 6.747470 6.614927 6.212890 7.016964 6.614927</span></span>
<span id="cb168-15"><a href="stochastic-explorations-using-mcmc.html#cb168-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [10,] 6.596912 6.468988 6.724836 6.596912 6.196374 6.997449 6.596912</span></span>
<span id="cb168-16"><a href="stochastic-explorations-using-mcmc.html#cb168-16" aria-hidden="true" tabindex="-1"></a><span class="do">## [11,] 7.032834 6.877582 7.188087 7.032834 6.622750 7.442918 7.032834</span></span>
<span id="cb168-17"><a href="stochastic-explorations-using-mcmc.html#cb168-17" aria-hidden="true" tabindex="-1"></a><span class="do">## [12,] 6.581822 6.462326 6.701317 6.581822 6.183896 6.979748 6.581822</span></span>
<span id="cb168-18"><a href="stochastic-explorations-using-mcmc.html#cb168-18" aria-hidden="true" tabindex="-1"></a><span class="do">## [13,] 6.467921 6.281998 6.653843 6.467921 6.045271 6.890571 6.467921</span></span>
<span id="cb168-19"><a href="stochastic-explorations-using-mcmc.html#cb168-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [14,] 6.566239 6.403813 6.728664 6.566239 6.153385 6.979092 6.566239</span></span>
<span id="cb168-20"><a href="stochastic-explorations-using-mcmc.html#cb168-20" aria-hidden="true" tabindex="-1"></a><span class="do">## [15,] 6.550129 6.388987 6.711270 6.550129 6.137779 6.962479 6.550129</span></span>
<span id="cb168-21"><a href="stochastic-explorations-using-mcmc.html#cb168-21" aria-hidden="true" tabindex="-1"></a><span class="do">## [16,] 6.888592 6.746097 7.031087 6.888592 6.483166 7.294019 6.888592</span></span>
<span id="cb168-22"><a href="stochastic-explorations-using-mcmc.html#cb168-22" aria-hidden="true" tabindex="-1"></a><span class="do">## [17,] 6.252735 6.063944 6.441526 6.252735 5.828815 6.676654 6.252735</span></span>
<span id="cb168-23"><a href="stochastic-explorations-using-mcmc.html#cb168-23" aria-hidden="true" tabindex="-1"></a><span class="do">## [18,] 6.795764 6.564634 7.026895 6.795764 6.351369 7.240160 6.795764</span></span>
<span id="cb168-24"><a href="stochastic-explorations-using-mcmc.html#cb168-24" aria-hidden="true" tabindex="-1"></a><span class="do">## [19,] 6.945687 6.766289 7.125086 6.945687 6.525866 7.365508 6.945687</span></span>
<span id="cb168-25"><a href="stochastic-explorations-using-mcmc.html#cb168-25" aria-hidden="true" tabindex="-1"></a><span class="do">## [20,] 7.000331 6.840374 7.160289 7.000331 6.588442 7.412220 7.000331</span></span>
<span id="cb168-26"><a href="stochastic-explorations-using-mcmc.html#cb168-26" aria-hidden="true" tabindex="-1"></a><span class="do">## [...]</span></span></code></pre></div>
<p>The option <code>estimator = "BPM</code> is not yet available in <code>coef()</code>, so we will need to work a little harder to get the coefficients by refitting the BPM.
First we need to extract a vector of zeros and ones representing which variables are included in the BPM model.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="stochastic-explorations-using-mcmc.html#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract a binary vector of zeros and ones for the variables included </span></span>
<span id="cb169-2"><a href="stochastic-explorations-using-mcmc.html#cb169-2" aria-hidden="true" tabindex="-1"></a><span class="co"># in the BPM</span></span>
<span id="cb169-3"><a href="stochastic-explorations-using-mcmc.html#cb169-3" aria-hidden="true" tabindex="-1"></a>BPM <span class="ot">=</span> <span class="fu">as.vector</span>(<span class="fu">which.matrix</span>(crime.ZS<span class="sc">$</span>which[crime.BPM<span class="sc">$</span>best],</span>
<span id="cb169-4"><a href="stochastic-explorations-using-mcmc.html#cb169-4" aria-hidden="true" tabindex="-1"></a>                             crime.ZS<span class="sc">$</span>n.vars))</span>
<span id="cb169-5"><a href="stochastic-explorations-using-mcmc.html#cb169-5" aria-hidden="true" tabindex="-1"></a>BPM</span></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0</code></pre>
<p>Next, we will refit the model with <code>bas.lm</code> using the optional argument <code>bestmodel = BPM</code>. In general, this is the starting model for the stochastic search, which is helpful for starting the MCMC.
We will also specify that want to have 1 model by setting <code>n.models = 1</code>. In this way, <code>bas.lm</code> starts with the BPM and fits only that model.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="stochastic-explorations-using-mcmc.html#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-run regression and specify `bestmodel` and `n.models`</span></span>
<span id="cb171-2"><a href="stochastic-explorations-using-mcmc.html#cb171-2" aria-hidden="true" tabindex="-1"></a>crime.ZS.BPM <span class="ot">=</span> <span class="fu">bas.lm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> UScrime,</span>
<span id="cb171-3"><a href="stochastic-explorations-using-mcmc.html#cb171-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">prior =</span> <span class="st">&quot;ZS-null&quot;</span>,</span>
<span id="cb171-4"><a href="stochastic-explorations-using-mcmc.html#cb171-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">modelprior =</span> <span class="fu">uniform</span>(),</span>
<span id="cb171-5"><a href="stochastic-explorations-using-mcmc.html#cb171-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">bestmodel =</span> BPM, <span class="at">n.models =</span> <span class="dv">1</span>)</span></code></pre></div>
<p>Now since we have only one model in our new object representing the BPM, we can use the <code>coef</code> function to obtain the summaries.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="stochastic-explorations-using-mcmc.html#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Obtain coefficients of MPM</span></span>
<span id="cb172-2"><a href="stochastic-explorations-using-mcmc.html#cb172-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(crime.ZS.BPM)</span></code></pre></div>
<pre><code>## 
##  Marginal Posterior Summaries of Coefficients: 
## 
##  Using  BMA 
## 
##  Based on the top  1 models 
##            post mean  post SD   post p(B != 0)
## Intercept   6.72494    0.02795   1.00000      
## M           1.28189    0.49219   1.00000      
## So          0.09028    0.11935   1.00000      
## Ed          2.24197    0.54029   1.00000      
## Po1         0.70543    0.75949   1.00000      
## Po2         0.16669    0.76781   1.00000      
## LF          0.00000    0.00000   0.00000      
## M.F         0.55521    1.22456   1.00000      
## Pop         0.00000    0.00000   0.00000      
## NW          0.06649    0.04244   1.00000      
## U1          0.00000    0.00000   0.00000      
## U2          0.28567    0.13836   1.00000      
## GDP         0.00000    0.00000   0.00000      
## Ineq        1.15756    0.30841   1.00000      
## Prob       -0.21012    0.07452   1.00000      
## Time        0.00000    0.00000   0.00000</code></pre>
<p>Note the posterior probabilities that coefficients are zero is either zero or one since we have selected a model.</p>
<p><strong>Comparison of Models</strong></p>
<p>After discussing all 4 different models, let us compare their prediction results.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="stochastic-explorations-using-mcmc.html#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set plot settings</span></span>
<span id="cb174-2"><a href="stochastic-explorations-using-mcmc.html#cb174-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">cex =</span> <span class="fl">1.8</span>, <span class="at">cex.axis =</span> <span class="fl">1.8</span>, <span class="at">cex.lab =</span> <span class="dv">2</span>, <span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb174-3"><a href="stochastic-explorations-using-mcmc.html#cb174-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">col.lab =</span> <span class="st">&quot;darkgrey&quot;</span>, <span class="at">col.axis =</span> <span class="st">&quot;darkgrey&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb174-4"><a href="stochastic-explorations-using-mcmc.html#cb174-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-5"><a href="stochastic-explorations-using-mcmc.html#cb174-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library and plot paired-correlations</span></span>
<span id="cb174-6"><a href="stochastic-explorations-using-mcmc.html#cb174-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb174-7"><a href="stochastic-explorations-using-mcmc.html#cb174-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(<span class="fu">data.frame</span>(<span class="at">HPM =</span> <span class="fu">as.vector</span>(crime.HPM<span class="sc">$</span>fit),  </span>
<span id="cb174-8"><a href="stochastic-explorations-using-mcmc.html#cb174-8" aria-hidden="true" tabindex="-1"></a>                   <span class="at">MPM =</span> <span class="fu">as.vector</span>(crime.MPM<span class="sc">$</span>fit),  </span>
<span id="cb174-9"><a href="stochastic-explorations-using-mcmc.html#cb174-9" aria-hidden="true" tabindex="-1"></a>                   <span class="at">BPM =</span> <span class="fu">as.vector</span>(crime.BPM<span class="sc">$</span>fit),  </span>
<span id="cb174-10"><a href="stochastic-explorations-using-mcmc.html#cb174-10" aria-hidden="true" tabindex="-1"></a>                   <span class="at">BMA =</span> <span class="fu">as.vector</span>(crime.BMA<span class="sc">$</span>fit))) </span></code></pre></div>
<p><img src="08-MCMC-04-decisions_files/figure-html/paired-cor-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>From the above paired correlation plots, we see that the correlations among them are extremely high. As expected, the single best predictive model (BPM) has the highest correlation with MPM, with a correlation of 0.998. However, the highest posterior model (HPM) and the Bayesian model averaging model (BMA) are nearly equally as good.</p>
</div>
<div id="prediction-with-new-data" class="section level3 hasAnchor" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Prediction with New Data<a href="stochastic-explorations-using-mcmc.html#prediction-with-new-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <code>newdata</code> option in the <code>predict</code> function, we can obtain prediction from a new data set. Here we pretend that <code>UScrime</code> is an another new data set, and we use BMA to obtain the prediction of new observations. Here we only show the results of the first 20 data points.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="stochastic-explorations-using-mcmc.html#cb175-1" aria-hidden="true" tabindex="-1"></a>BMA.new <span class="ot">=</span> <span class="fu">predict</span>(crime.ZS, <span class="at">newdata =</span> UScrime, <span class="at">estimator =</span> <span class="st">&quot;BMA&quot;</span>,</span>
<span id="cb175-2"><a href="stochastic-explorations-using-mcmc.html#cb175-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">se.fit =</span> <span class="cn">TRUE</span>, <span class="at">nsim =</span> <span class="dv">10000</span>)</span>
<span id="cb175-3"><a href="stochastic-explorations-using-mcmc.html#cb175-3" aria-hidden="true" tabindex="-1"></a>crime.conf.fit.new <span class="ot">=</span> <span class="fu">confint</span>(BMA.new, <span class="at">parm =</span> <span class="st">&quot;mean&quot;</span>)</span>
<span id="cb175-4"><a href="stochastic-explorations-using-mcmc.html#cb175-4" aria-hidden="true" tabindex="-1"></a>crime.conf.pred.new <span class="ot">=</span> <span class="fu">confint</span>(BMA.new, <span class="at">parm =</span> <span class="st">&quot;pred&quot;</span>)</span>
<span id="cb175-5"><a href="stochastic-explorations-using-mcmc.html#cb175-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-6"><a href="stochastic-explorations-using-mcmc.html#cb175-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the combined results compared to the fitted values in BPM</span></span>
<span id="cb175-7"><a href="stochastic-explorations-using-mcmc.html#cb175-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cbind</span>(crime.BPM<span class="sc">$</span>fit, crime.conf.fit.new, crime.conf.pred.new)</span>
<span id="cb175-8"><a href="stochastic-explorations-using-mcmc.html#cb175-8" aria-hidden="true" tabindex="-1"></a><span class="do">##                    2.5%    97.5%     mean     2.5%    97.5%     pred</span></span>
<span id="cb175-9"><a href="stochastic-explorations-using-mcmc.html#cb175-9" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1,] 6.668988 6.516780 6.821115 6.661770 6.242610 7.067277 6.661770</span></span>
<span id="cb175-10"><a href="stochastic-explorations-using-mcmc.html#cb175-10" aria-hidden="true" tabindex="-1"></a><span class="do">##  [2,] 7.290854 7.135519 7.456165 7.298827 6.860438 7.711159 7.298827</span></span>
<span id="cb175-11"><a href="stochastic-explorations-using-mcmc.html#cb175-11" aria-hidden="true" tabindex="-1"></a><span class="do">##  [3,] 6.202166 5.957311 6.398369 6.179308 5.719796 6.610824 6.179308</span></span>
<span id="cb175-12"><a href="stochastic-explorations-using-mcmc.html#cb175-12" aria-hidden="true" tabindex="-1"></a><span class="do">##  [4,] 7.661307 7.376774 7.830490 7.610585 7.143189 8.037652 7.610585</span></span>
<span id="cb175-13"><a href="stochastic-explorations-using-mcmc.html#cb175-13" aria-hidden="true" tabindex="-1"></a><span class="do">##  [5,] 7.015570 6.849270 7.254401 7.054238 6.622239 7.482155 7.054238</span></span>
<span id="cb175-14"><a href="stochastic-explorations-using-mcmc.html#cb175-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  [6,] 6.469547 6.291613 6.753466 6.514064 6.060883 6.954264 6.514064</span></span>
<span id="cb175-15"><a href="stochastic-explorations-using-mcmc.html#cb175-15" aria-hidden="true" tabindex="-1"></a><span class="do">##  [7,] 6.776133 6.506525 7.076572 6.784846 6.296845 7.257308 6.784846</span></span>
<span id="cb175-16"><a href="stochastic-explorations-using-mcmc.html#cb175-16" aria-hidden="true" tabindex="-1"></a><span class="do">##  [8,] 7.299560 7.044601 7.480467 7.266344 6.834868 7.713212 7.266344</span></span>
<span id="cb175-17"><a href="stochastic-explorations-using-mcmc.html#cb175-17" aria-hidden="true" tabindex="-1"></a><span class="do">##  [9,] 6.614927 6.489487 6.789850 6.629448 6.227549 7.043140 6.629448</span></span>
<span id="cb175-18"><a href="stochastic-explorations-using-mcmc.html#cb175-18" aria-hidden="true" tabindex="-1"></a><span class="do">## [10,] 6.596912 6.467658 6.739482 6.601246 6.177264 7.008391 6.601246</span></span>
<span id="cb175-19"><a href="stochastic-explorations-using-mcmc.html#cb175-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [11,] 7.032834 6.868583 7.234648 7.055003 6.614423 7.471305 7.055003</span></span>
<span id="cb175-20"><a href="stochastic-explorations-using-mcmc.html#cb175-20" aria-hidden="true" tabindex="-1"></a><span class="do">## [12,] 6.581822 6.423730 6.718106 6.570625 6.154369 6.983410 6.570625</span></span>
<span id="cb175-21"><a href="stochastic-explorations-using-mcmc.html#cb175-21" aria-hidden="true" tabindex="-1"></a><span class="do">## [13,] 6.467921 6.218146 6.723079 6.472327 6.012763 6.946144 6.472327</span></span>
<span id="cb175-22"><a href="stochastic-explorations-using-mcmc.html#cb175-22" aria-hidden="true" tabindex="-1"></a><span class="do">## [14,] 6.566239 6.391846 6.762702 6.582374 6.142590 6.996450 6.582374</span></span>
<span id="cb175-23"><a href="stochastic-explorations-using-mcmc.html#cb175-23" aria-hidden="true" tabindex="-1"></a><span class="do">## [15,] 6.550129 6.349988 6.751031 6.556880 6.114315 6.983550 6.556880</span></span>
<span id="cb175-24"><a href="stochastic-explorations-using-mcmc.html#cb175-24" aria-hidden="true" tabindex="-1"></a><span class="do">## [16,] 6.888592 6.744616 7.062936 6.905017 6.491021 7.323912 6.905017</span></span>
<span id="cb175-25"><a href="stochastic-explorations-using-mcmc.html#cb175-25" aria-hidden="true" tabindex="-1"></a><span class="do">## [17,] 6.252735 5.991623 6.467533 6.229073 5.768655 6.680241 6.229073</span></span>
<span id="cb175-26"><a href="stochastic-explorations-using-mcmc.html#cb175-26" aria-hidden="true" tabindex="-1"></a><span class="do">## [18,] 6.795764 6.554739 7.103414 6.809572 6.348136 7.290984 6.809572</span></span>
<span id="cb175-27"><a href="stochastic-explorations-using-mcmc.html#cb175-27" aria-hidden="true" tabindex="-1"></a><span class="do">## [19,] 6.945687 6.750989 7.133344 6.943294 6.508221 7.370331 6.943294</span></span>
<span id="cb175-28"><a href="stochastic-explorations-using-mcmc.html#cb175-28" aria-hidden="true" tabindex="-1"></a><span class="do">## [20,] 7.000331 6.775472 7.138530 6.961980 6.529950 7.388461 6.961980</span></span>
<span id="cb175-29"><a href="stochastic-explorations-using-mcmc.html#cb175-29" aria-hidden="true" tabindex="-1"></a><span class="do">## [...]</span></span></code></pre></div>
</div>
</div>
<div id="summary-4" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Summary<a href="stochastic-explorations-using-mcmc.html#summary-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we have introduced one of the common stochastic exploration methods, Markov Chain Monte Carlo, to explore the model space to obtain approximation of posterior probability of each model when the model space is too large for theoretical enumeration. We see that model selection can be sensitive to the prior distributions of coefficients, and introduced Zellner’s <span class="math inline">\(g\)</span>-prior so that we have to elicit only one hyper-parameter to specify the prior. Still model selection can be sensitive to the choice of <span class="math inline">\(g\)</span> where values that are too large may unintentially lead to the null model receiving high probability in Bartlett’s paradox. To resolve this and other paradoxes related to the choice of <span class="math inline">\(g\)</span>, we recommend default choices that have improved robustness to prior misspecification such as the unit information <span class="math inline">\(g\)</span>-prior, the Zellner-Siow Cauchy prior, and the hyper-<span class="math inline">\(g/n\)</span> prior.</p>
<p>We then demonstrated a multiple linear regression process using the <code>BAS</code> package and the US crime data <code>UScrime</code> using the Zellner-Siow cauchy prior, and have tried to understand the importance of variables.
Finally, we have compared the prediction results from different models, such as the ones from Bayesian model average (BMA), the highest probability model (HPM), the median probability model (MPM), and the best predictive model (BPM). For the comparison, we have used the Zellner-Siow Cauchy prior. But of course there is not one single best prior that is the best overall. If you do have prior information about a variable, you should include it. If you expect that there should be many predictors related to the response variable <span class="math inline">\(Y\)</span>, but that each has a small effect, an alternate prior may be better. Also, think critically about whether model selection is important. If you believe that all the variables should be relevant but are worried about over fitting, there are alternative priors that will avoid putting probabilities on coefficients that are exactly zero and will still prevent over fitting by shrinkage of coefficients to prior means. Examples include the Bayesian lasso or Bayesian horseshoe.</p>
<p>There are other forms of model uncertainty that you may want to consider, such as linearity in the relationship between the predictors and the response, uncertainty about the presence of outliers, and uncertainty about the distribution of the response. These forms of uncertainty can be incorporated by expanding the models and priors similar to what we have covered here.</p>
<p>Multiple linear regression is one of the most widely used statistical methods, however, this is just the tip of the iceberg of what you can do with Bayesian methods.</p>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Chaloner, Kathryn, and Rollin Brant. 1988. <span>“A Bayesian Approach to Outlier Detection and Residual Analysis.”</span> <em>Biometrika</em> 75 (4): 651–59.
</div>
<div class="csl-entry">
Hoff, Peter D. 2009. <em>A First Course in Bayesian Statistical Methods</em>. Springer Science &amp; Business Media.
</div>
<div class="csl-entry">
Jeffreys, Sir Harold. 1961. <em>Theory of Probability: 3rd Edition</em>. Clarendon Press.
</div>
<div class="csl-entry">
Kass, Robert E, and Adrian E Raftery. 1995. <span>“Bayes Factors.”</span> <em>Journal of the American Statistical Association</em> 90 (430): 773–95.
</div>
<div class="csl-entry">
Venables, William N, and Brian D Ripley. 2013. <em>Modern Applied Statistics with s-PLUS</em>. Springer Science &amp; Business Media.
</div>
</div>
</div>
</div>












<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-venables2013modern" class="csl-entry">
Venables, William N, and Brian D Ripley. 2013. <em>Modern Applied Statistics with s-PLUS</em>. Springer Science &amp; Business Media.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>More details can be found in <span class="citation">Venables and Ripley (<a href="#ref-venables2013modern" role="doc-biblioref">2013</a>)</span>.<a href="stochastic-explorations-using-mcmc.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-model-choice.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/statswithr/book/edit/master/08-MCMC-00-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
