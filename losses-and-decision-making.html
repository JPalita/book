<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Losses and Decision Making | An Introduction to Bayesian Thinking</title>
  <meta name="description" content="Chapter 3 Losses and Decision Making | An Introduction to Bayesian Thinking" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Losses and Decision Making | An Introduction to Bayesian Thinking" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="StatsWithR/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Losses and Decision Making | An Introduction to Bayesian Thinking" />
  
  
  

<meta name="author" content="Merlise Clyde" />
<meta name="author" content="Mine Çetinkaya-Rundel" />
<meta name="author" content="Colin Rundel" />
<meta name="author" content="David Banks" />
<meta name="author" content="Christine Chai" />
<meta name="author" content="Lizzy Huang" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-inference.html"/>
<link rel="next" href="inference-and-decision-making-with-multiple-parameters.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html"><i class="fa fa-check"></i><b>1</b> The Basics of Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayes-rule"><i class="fa fa-check"></i><b>1.1</b> Bayes’ Rule</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#sec:bayes-rule"><i class="fa fa-check"></i><b>1.1.1</b> Conditional Probabilities &amp; Bayes’ Rule</a></li>
<li class="chapter" data-level="1.1.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#sec:diagnostic-testing"><i class="fa fa-check"></i><b>1.1.2</b> Bayes’ Rule and Diagnostic Testing</a></li>
<li class="chapter" data-level="1.1.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayes-updating"><i class="fa fa-check"></i><b>1.1.3</b> Bayes Updating</a></li>
<li class="chapter" data-level="1.1.4" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#bayesian-vs.-frequentist-definitions-of-probability"><i class="fa fa-check"></i><b>1.1.4</b> Bayesian vs. Frequentist Definitions of Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion"><i class="fa fa-check"></i><b>1.2</b> Inference for a Proportion</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion-frequentist-approach"><i class="fa fa-check"></i><b>1.2.1</b> Inference for a Proportion: Frequentist Approach</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#inference-for-a-proportion-bayesian-approach"><i class="fa fa-check"></i><b>1.2.2</b> Inference for a Proportion: Bayesian Approach</a></li>
<li class="chapter" data-level="1.2.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#effect-of-sample-size-on-the-posterior"><i class="fa fa-check"></i><b>1.2.3</b> Effect of Sample Size on the Posterior</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#frequentist-vs.-bayesian-inference"><i class="fa fa-check"></i><b>1.3</b> Frequentist vs. Bayesian Inference</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#frequentist-vs.-bayesian-inference-1"><i class="fa fa-check"></i><b>1.3.1</b> Frequentist vs. Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-bayesian-statistics.html"><a href="the-basics-of-bayesian-statistics.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>2</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#continuous-variables-and-eliciting-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Continuous Variables and Eliciting Probability Distributions</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#from-the-discrete-to-the-continuous"><i class="fa fa-check"></i><b>2.1.1</b> From the Discrete to the Continuous</a></li>
<li class="chapter" data-level="2.1.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#elicitation"><i class="fa fa-check"></i><b>2.1.2</b> Elicitation</a></li>
<li class="chapter" data-level="2.1.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#conjugacy"><i class="fa fa-check"></i><b>2.1.3</b> Conjugacy</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#three-conjugate-families"><i class="fa fa-check"></i><b>2.2</b> Three Conjugate Families</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#inference-on-a-binomial-proportion"><i class="fa fa-check"></i><b>2.2.1</b> Inference on a Binomial Proportion</a></li>
<li class="chapter" data-level="2.2.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#the-gamma-poisson-conjugate-families"><i class="fa fa-check"></i><b>2.2.2</b> The Gamma-Poisson Conjugate Families</a></li>
<li class="chapter" data-level="2.2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#sec:normal-normal"><i class="fa fa-check"></i><b>2.2.3</b> The Normal-Normal Conjugate Families</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals-and-predictive-inference"><i class="fa fa-check"></i><b>2.3</b> Credible Intervals and Predictive Inference</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#non-conjugate-priors"><i class="fa fa-check"></i><b>2.3.1</b> Non-Conjugate Priors</a></li>
<li class="chapter" data-level="2.3.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#credible-intervals"><i class="fa fa-check"></i><b>2.3.2</b> Credible Intervals</a></li>
<li class="chapter" data-level="2.3.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#predictive-inference"><i class="fa fa-check"></i><b>2.3.3</b> Predictive Inference</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html"><i class="fa fa-check"></i><b>3</b> Losses and Decision Making</a>
<ul>
<li class="chapter" data-level="3.1" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#bayesian-decision-making"><i class="fa fa-check"></i><b>3.1</b> Bayesian Decision Making</a></li>
<li class="chapter" data-level="3.2" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#loss-functions"><i class="fa fa-check"></i><b>3.2</b> Loss Functions</a></li>
<li class="chapter" data-level="3.3" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#working-with-loss-functions"><i class="fa fa-check"></i><b>3.3</b> Working with Loss Functions</a></li>
<li class="chapter" data-level="3.4" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#minimizing-expected-loss-for-hypothesis-testing"><i class="fa fa-check"></i><b>3.4</b> Minimizing Expected Loss for Hypothesis Testing</a></li>
<li class="chapter" data-level="3.5" data-path="losses-and-decision-making.html"><a href="losses-and-decision-making.html#sec:bayes-factors"><i class="fa fa-check"></i><b>3.5</b> Posterior Probabilities of Hypotheses and Bayes Factors</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html"><i class="fa fa-check"></i><b>4</b> Inference and Decision-Making with Multiple Parameters</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:normal-gamma"><i class="fa fa-check"></i><b>4.1</b> The Normal-Gamma Conjugate Family</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#conjugate-prior-for-mu-and-sigma2"><i class="fa fa-check"></i><b>4.1.1</b> Conjugate Prior for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="4.1.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#conjugate-posterior-distribution"><i class="fa fa-check"></i><b>4.1.2</b> Conjugate Posterior Distribution</a></li>
<li class="chapter" data-level="4.1.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#marginal-distribution-for-mu-student-t"><i class="fa fa-check"></i><b>4.1.3</b> Marginal Distribution for <span class="math inline">\(\mu\)</span>: Student <span class="math inline">\(t\)</span></a></li>
<li class="chapter" data-level="4.1.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#credible-intervals-for-mu"><i class="fa fa-check"></i><b>4.1.4</b> Credible Intervals for <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="4.1.5" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:tapwater"><i class="fa fa-check"></i><b>4.1.5</b> Example: TTHM in Tapwater</a></li>
<li class="chapter" data-level="4.1.6" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#section-summary"><i class="fa fa-check"></i><b>4.1.6</b> Section Summary</a></li>
<li class="chapter" data-level="4.1.7" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#optional-derivations"><i class="fa fa-check"></i><b>4.1.7</b> (Optional) Derivations</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-MC"><i class="fa fa-check"></i><b>4.2</b> Monte Carlo Inference</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#monte-carlo-sampling"><i class="fa fa-check"></i><b>4.2.1</b> Monte Carlo Sampling</a></li>
<li class="chapter" data-level="4.2.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#monte-carlo-inference-tap-water-example"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Inference: Tap Water Example</a></li>
<li class="chapter" data-level="4.2.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#monte-carlo-inference-for-functions-of-parameters"><i class="fa fa-check"></i><b>4.2.3</b> Monte Carlo Inference for Functions of Parameters</a></li>
<li class="chapter" data-level="4.2.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#summary"><i class="fa fa-check"></i><b>4.2.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-predictive"><i class="fa fa-check"></i><b>4.3</b> Predictive Distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#prior-predictive-distribution"><i class="fa fa-check"></i><b>4.3.1</b> Prior Predictive Distribution</a></li>
<li class="chapter" data-level="4.3.2" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#tap-water-example-continued"><i class="fa fa-check"></i><b>4.3.2</b> Tap Water Example (continued)</a></li>
<li class="chapter" data-level="4.3.3" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sampling-from-the-prior-predictive-in-r"><i class="fa fa-check"></i><b>4.3.3</b> Sampling from the Prior Predictive in <code>R</code></a></li>
<li class="chapter" data-level="4.3.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#posterior-predictive"><i class="fa fa-check"></i><b>4.3.4</b> Posterior Predictive</a></li>
<li class="chapter" data-level="4.3.5" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#summary-1"><i class="fa fa-check"></i><b>4.3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-reference"><i class="fa fa-check"></i><b>4.4</b> Reference Priors</a></li>
<li class="chapter" data-level="4.5" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-Cauchy"><i class="fa fa-check"></i><b>4.5</b> Mixtures of Conjugate Priors</a></li>
<li class="chapter" data-level="4.6" data-path="inference-and-decision-making-with-multiple-parameters.html"><a href="inference-and-decision-making-with-multiple-parameters.html#sec:NG-MCMC"><i class="fa fa-check"></i><b>4.6</b> Markov Chain Monte Carlo (MCMC)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Testing with Normal Populations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#sec:known-var"><i class="fa fa-check"></i><b>5.1</b> Bayes Factors for Testing a Normal Mean: variance known</a></li>
<li class="chapter" data-level="5.2" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#comparing-two-paired-means-using-bayes-factors"><i class="fa fa-check"></i><b>5.2</b> Comparing Two Paired Means using Bayes Factors</a></li>
<li class="chapter" data-level="5.3" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#sec:indep-means"><i class="fa fa-check"></i><b>5.3</b> Comparing Independent Means: Hypothesis Testing</a></li>
<li class="chapter" data-level="5.4" data-path="hypothesis-testing-with-normal-populations.html"><a href="hypothesis-testing-with-normal-populations.html#inference-after-testing"><i class="fa fa-check"></i><b>5.4</b> Inference after Testing</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html"><i class="fa fa-check"></i><b>6</b> Introduction to Bayesian Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:simple-linear"><i class="fa fa-check"></i><b>6.1</b> Bayesian Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#frequentist-ordinary-least-square-ols-simple-linear-regression"><i class="fa fa-check"></i><b>6.1.1</b> Frequentist Ordinary Least Square (OLS) Simple Linear Regression</a></li>
<li class="chapter" data-level="6.1.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#bayesian-simple-linear-regression-using-the-reference-prior"><i class="fa fa-check"></i><b>6.1.2</b> Bayesian Simple Linear Regression Using the Reference Prior</a></li>
<li class="chapter" data-level="6.1.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:informative-prior"><i class="fa fa-check"></i><b>6.1.3</b> Informative Priors</a></li>
<li class="chapter" data-level="6.1.4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:derivations"><i class="fa fa-check"></i><b>6.1.4</b> (Optional) Derivations of Marginal Posterior Distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="6.1.5" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#marginal-posterior-distribution-of-beta"><i class="fa fa-check"></i><b>6.1.5</b> Marginal Posterior Distribution of <span class="math inline">\(\beta\)</span></a></li>
<li class="chapter" data-level="6.1.6" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#marginal-posterior-distribution-of-alpha"><i class="fa fa-check"></i><b>6.1.6</b> Marginal Posterior Distribution of <span class="math inline">\(\alpha\)</span></a></li>
<li class="chapter" data-level="6.1.7" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#marginal-posterior-distribution-of-sigma2"><i class="fa fa-check"></i><b>6.1.7</b> Marginal Posterior Distribution of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="6.1.8" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#joint-normal-gamma-posterior-distributions"><i class="fa fa-check"></i><b>6.1.8</b> Joint Normal-Gamma Posterior Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:Checking-outliers"><i class="fa fa-check"></i><b>6.2</b> Checking Outliers</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#posterior-distribution-of-epsilon_j-conditioning-on-sigma2"><i class="fa fa-check"></i><b>6.2.1</b> Posterior Distribution of <span class="math inline">\(\epsilon_j\)</span> Conditioning On <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="6.2.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#implementation-using-bas-package"><i class="fa fa-check"></i><b>6.2.2</b> Implementation Using <code>BAS</code> Package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#sec:Bayes-multiple-regression"><i class="fa fa-check"></i><b>6.3</b> Bayesian Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#the-model"><i class="fa fa-check"></i><b>6.3.1</b> The Model</a></li>
<li class="chapter" data-level="6.3.2" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#data-pre-processing"><i class="fa fa-check"></i><b>6.3.2</b> Data Pre-processing</a></li>
<li class="chapter" data-level="6.3.3" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#specify-bayesian-prior-distributions"><i class="fa fa-check"></i><b>6.3.3</b> Specify Bayesian Prior Distributions</a></li>
<li class="chapter" data-level="6.3.4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#fitting-the-bayesian-model"><i class="fa fa-check"></i><b>6.3.4</b> Fitting the Bayesian Model</a></li>
<li class="chapter" data-level="6.3.5" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#posterior-means-and-posterior-standard-deviations"><i class="fa fa-check"></i><b>6.3.5</b> Posterior Means and Posterior Standard Deviations</a></li>
<li class="chapter" data-level="6.3.6" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#credible-intervals-summary"><i class="fa fa-check"></i><b>6.3.6</b> Credible Intervals Summary</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="introduction-to-bayesian-regression.html"><a href="introduction-to-bayesian-regression.html#summary-2"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html"><i class="fa fa-check"></i><b>7</b> Bayesian Model Choice</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#sec:BIC"><i class="fa fa-check"></i><b>7.1</b> Bayesian Information Criterion (BIC)</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#definition-of-bic"><i class="fa fa-check"></i><b>7.1.1</b> Definition of BIC</a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#backward-elimination-with-bic"><i class="fa fa-check"></i><b>7.1.2</b> Backward Elimination with BIC</a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#coefficient-estimates-under-reference-prior-for-best-bic-model"><i class="fa fa-check"></i><b>7.1.3</b> Coefficient Estimates Under Reference Prior for Best BIC Model</a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#other-criteria"><i class="fa fa-check"></i><b>7.1.4</b> Other Criteria</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#sec:BMU"><i class="fa fa-check"></i><b>7.2</b> Bayesian Model Uncertainty</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#model-uncertainty"><i class="fa fa-check"></i><b>7.2.1</b> Model Uncertainty</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#calculating-posterior-probability-in-r"><i class="fa fa-check"></i><b>7.2.2</b> Calculating Posterior Probability in R</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#bayesian-model-averaging"><i class="fa fa-check"></i><b>7.3</b> Bayesian Model Averaging</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#visualizing-model-uncertainty"><i class="fa fa-check"></i><b>7.3.1</b> Visualizing Model Uncertainty</a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#bayesian-model-averaging-using-posterior-probability"><i class="fa fa-check"></i><b>7.3.2</b> Bayesian Model Averaging Using Posterior Probability</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#coefficient-summary-under-bma"><i class="fa fa-check"></i><b>7.3.3</b> Coefficient Summary under BMA</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian-model-choice.html"><a href="bayesian-model-choice.html#summary-3"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html"><i class="fa fa-check"></i><b>8</b> Stochastic Explorations Using MCMC</a>
<ul>
<li class="chapter" data-level="8.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#stochastic-exploration"><i class="fa fa-check"></i><b>8.1</b> Stochastic Exploration</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#markov-chain-monte-carlo-exploration"><i class="fa fa-check"></i><b>8.1.1</b> Markov Chain Monte Carlo Exploration</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#other-priors-for-bayesian-model-uncertainty"><i class="fa fa-check"></i><b>8.2</b> Other Priors for Bayesian Model Uncertainty</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#zellners-g-prior"><i class="fa fa-check"></i><b>8.2.1</b> Zellner’s <span class="math inline">\(g\)</span>-Prior</a></li>
<li class="chapter" data-level="8.2.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#bayes-factor-of-zellners-g-prior"><i class="fa fa-check"></i><b>8.2.2</b> Bayes Factor of Zellner’s <span class="math inline">\(g\)</span>-Prior</a></li>
<li class="chapter" data-level="8.2.3" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#kids-cognitive-score-example"><i class="fa fa-check"></i><b>8.2.3</b> Kid’s Cognitive Score Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#r-demo-on-bas-package"><i class="fa fa-check"></i><b>8.3</b> R Demo on <code>BAS</code> Package</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#the-uscrime-data-set-and-data-processing"><i class="fa fa-check"></i><b>8.3.1</b> The <code>UScrime</code> Data Set and Data Processing</a></li>
<li class="chapter" data-level="8.3.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#bayesian-models-and-diagnostics"><i class="fa fa-check"></i><b>8.3.2</b> Bayesian Models and Diagnostics</a></li>
<li class="chapter" data-level="8.3.3" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#posterior-uncertainty-in-coefficients"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Uncertainty in Coefficients</a></li>
<li class="chapter" data-level="8.3.4" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#prediction"><i class="fa fa-check"></i><b>8.3.4</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#decision-making-under-model-uncertainty"><i class="fa fa-check"></i><b>8.4</b> Decision Making Under Model Uncertainty</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#model-choice"><i class="fa fa-check"></i><b>8.4.1</b> Model Choice</a></li>
<li class="chapter" data-level="8.4.2" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#prediction-with-new-data"><i class="fa fa-check"></i><b>8.4.2</b> Prediction with New Data</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="stochastic-explorations-using-mcmc.html"><a href="stochastic-explorations-using-mcmc.html#summary-4"><i class="fa fa-check"></i><b>8.5</b> Summary</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Thinking</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="losses-and-decision-making" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Losses and Decision Making<a href="losses-and-decision-making.html#losses-and-decision-making" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the previous chapter, we learned about continuous random variables. That enabled us to study conjugate families, such as the beta binomial, the Poisson gamma, and the normal normal. We also considered the difficulties of eliciting a personal prior, and of handling inference in nonconjugate cases. Finally, we introduced the credible interval and studied predictive inference.</p>
<p>In this new chapter, we will introduce loss functions and Bayesian decision making, minimizing expected loss for hypothesis testing, and define posterior probabilities of hypothesis and Bayes factors. We will then outline Bayesian testing for two proportions and two means, discuss how findings from credible intervals compare to those from our hypothesis test, and finally discuss when to reject, accept, or wait.</p>

<div id="bayesian-decision-making" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Bayesian Decision Making<a href="losses-and-decision-making.html#bayesian-decision-making" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To a Bayesian, the posterior distribution is the basis of any inference, since it integrates both his/her prior opinions and knowledge and the new information provided by the data. It also contains everything she believes about the distribution of the unknown parameter of interest.</p>
<p>However, the posterior distribution on its own is not always sufficient. Sometimes the inference we want to express is a <strong>credible interval</strong>, because it indicates a range of likely values for the parameter. That would be helpful if you wanted to say that you are <strong>95% certain</strong> the probability of an RU-486 pregnancy lies between some number <span class="math inline">\(L\)</span> and some number <span class="math inline">\(U\)</span>. And on other occasions, one needs to make a single number guess about the value of the parameter. For example, you might want to declare the average payoff for an insurance claim or tell a patient how much longer he/she has to live.</p>
<p>Therefore, the Bayesian perspective leads directly to <strong>decision theory</strong>. And in decision theory, one seeks to minimize one’s expected loss.</p>

</div>
<div id="loss-functions" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Loss Functions<a href="losses-and-decision-making.html#loss-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Quantifying the loss can be tricky, and Table <a href="losses-and-decision-making.html#tab:loss-functions">3.1</a> summarizes three different examples with three different loss functions.</p>
<p>If you’re declaring the average payoff for an insurance claim, and if you are <strong>linear</strong> in how you value money, that is, twice as much money is exactly twice as good, then one can prove that the optimal one-number estimate is the <strong>median</strong> of the posterior distribution. But in different situations, other measures of loss may apply.</p>
<p>If you are advising a patient on his/her life expectancy, it is easy to imagine that large errors are far more problematic than small ones. And perhaps the loss increases as the <strong>square</strong> of how far off your single number estimate is from the truth. For example, if she is told that her average life expectancy is two years, and it is actually ten, then her estate planning will be catastrophically bad, and she will die in poverty. In the case when the loss is proportional to the <strong>quadratic</strong> error, one can show that the optimal one-number estimate is the <strong>mean</strong> of the posterior distribution.</p>
<p>Finally, in some cases, the penalty is 0 if you are exactly correct, but constant if you’re at all wrong. This is the case with the old saying that close only counts with horseshoes and hand grenades; i.e., coming close but not succeeding is not good enough. And it would apply if you want a prize for correctly guessing the number of jelly beans in a jar. Here, of course, instead of minimizing expected losses, we want to <strong>maximize the expected gain</strong>. If a Bayesian is in such a situation, then his/her best one-number estimate is the <strong>mode</strong> of his/her posterior distribution, which is the most likely value.</p>
<p>There is a large literature on decision theory, and it is directly linked to risk analysis, which arises in many fields. Although it is possible for frequentists to employ a certain kind of decision theory, it is much more natural for Bayesians.</p>
<table>
<caption><span id="tab:loss-functions">Table 3.1: </span>Loss Functions</caption>
<thead>
<tr class="header">
<th align="center">Loss</th>
<th align="center">Best Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Linear</td>
<td align="center">Median</td>
</tr>
<tr class="even">
<td align="center">Quadratic</td>
<td align="center">Mean</td>
</tr>
<tr class="odd">
<td align="center">0/1</td>
<td align="center">Mode</td>
</tr>
</tbody>
</table>
<p>When making point estimates of unknown parameters, we should make the choices that minimize the loss. Nevertheless, the best estimate depends on the kind of loss function we are using, and we will discuss in more depth how these best estimates are determined in the next section.</p>

</div>
<div id="working-with-loss-functions" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Working with Loss Functions<a href="losses-and-decision-making.html#working-with-loss-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now we illustrate why certain estimates minimize certain loss functions.</p>
<div class="example">
<p><span id="exm:car" class="example"><strong>Example 3.1  </strong></span>You work at a car dealership. Your boss wants to know how many cars the dealership will sell per month. An analyst who has worked with past data from your company provided you a distribution that shows the probability of number of cars the dealership will sell per month. In Bayesian lingo, this is called the posterior distribution. A dot plot of that posterior is shown in Figure <a href="losses-and-decision-making.html#fig:posterior-decision">3.1</a>. The mean, median and the mode of the distribution are also marked on the plot. Your boss does not know any Bayesian statistics though, so he/she wants you to report <strong>a single number</strong> for the number of cars the dealership will sell per month.</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:posterior-decision"></span>
<img src="03-decision-03-working_files/figure-html/posterior-decision-1.png" alt="Posterior" width="960" />
<p class="caption">
Figure 3.1: Posterior
</p>
</div>
<p>Suppose your single guess is 30, and we call this <span class="math inline">\(g\)</span> in the following calculations. If your loss function is <span class="math inline">\(L_0\)</span> (i.e., a 0/1 loss), then you lose a point for each value in your posterior that differs from your guess and do not lose any points for values that exactly equal your guess. The total loss is the sum of the losses from each value in the posterior.</p>
<p>In mathematical terms, we define <span class="math inline">\(L_0\)</span> (0/1 loss) as</p>
<p><span class="math display">\[L_{0,i}(0,g) = \left\{ \begin{array}{cc}
0 &amp; \text{if } g=x_i \\ 1 &amp; \text{otherwise}
\end{array}\right.\]</span></p>
<p>The total loss is <span class="math inline">\(L_0 = \sum_i L_{0,i}(0,g)\)</span>.</p>
<p>Let’s calculate what the total loss would be if your guess is 30. Table <a href="losses-and-decision-making.html#tab:L0-table">3.2</a> summarizes the values in the posterior distribution sorted in descending order.</p>
<p>The first value is 4, which is not equal to your guess of 30, so the loss for that value is 1. The second value is 19, also not equal to your guess of 30, and the loss for that value is also 1. The third value is 20, also not equal to your guess of 30, and the loss for this value is also 1.</p>
<p>There is only one 30 in your posterior, and the loss for this value is 0 – since it’s equal to your guess (good news!). The remaining values in the posterior are all different than 30 hence, the loss for them are all ones as well.</p>
<p>To find the total loss, we simply sum over these individual losses in the posterior distribution with 51 observations where only one of them equals our guess and the remainder are different. Hence, the total loss is 50.</p>
<p>Figure <a href="losses-and-decision-making.html#fig:L0-mode">3.2</a> is a visualization of the posterior distribution, along with the 0-1 loss calculated for a series of possible guesses within the range of the posterior distribution. To create this visualization of the loss function, we went through the process we described earlier for a guess of 30 for all guesses considered, and we recorded the total loss. We can see that the loss function has the lowest value when <span class="math inline">\(g\)</span>, our guess, is equal to <strong>the most frequent observation</strong> in the posterior. Hence, <span class="math inline">\(L_0\)</span> is minimized at the <strong>mode</strong> of the posterior, which means that if we use the 0/1 loss, the best point estimate is the mode of the posterior.</p>
<table>
<caption><span id="tab:L0-table">Table 3.2: </span>L0: 0/1 loss for g = 30</caption>
<thead>
<tr class="header">
<th align="center">i</th>
<th align="center">x_i</th>
<th align="center">L0: 0/1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">4</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">19</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">20</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="odd">
<td align="center">14</td>
<td align="center">30</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="odd">
<td align="center">50</td>
<td align="center">47</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">51</td>
<td align="center">49</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center">Total</td>
<td align="center">50</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:L0-mode"></span>
<img src="03-decision-03-working_files/figure-html/L0-mode-1.png" alt="L0 is minimized at the mode of the posterior" width="960" />
<p class="caption">
Figure 3.2: L0 is minimized at the mode of the posterior
</p>
</div>
<p>Let’s consider another loss function. If your loss function is <span class="math inline">\(L_1\)</span> (i.e., linear loss), then the total loss for a guess is the sum of the <strong>absolute values</strong> of the difference between that guess and each value in the posterior. Note that the absolute value function is required, because overestimates and underestimates do not cancel out.</p>
<p>In mathematical terms, <span class="math inline">\(L_1\)</span> (linear loss) is calculated as <span class="math inline">\(L_1(g) = \sum_i |x_i - g|\)</span>.</p>
<p>We can once again calculate the total loss under <span class="math inline">\(L_1\)</span> if your guess is 30. Table <a href="losses-and-decision-making.html#tab:L1-table">3.3</a> summarizes the values in the posterior distribution sorted in descending order.</p>
<p>The first value is 4, and the absolute value of the difference between 4 and 30 is 26. The second value is 19, and the absolute value of the difference between 19 and 30 is 11. The third value is 20 and the absolute value of the difference between 20 and 30 is 10.</p>
<p>There is only one 30 in your posterior, and the loss for this value is 0 since it is equal to your guess. The remaining value in the posterior are all different than 30 hence their losses are different than 0.</p>
<p>To find the total loss, we again simply sum over these individual losses, and the total is to 346.</p>
<p>Again, Figure <a href="losses-and-decision-making.html#fig:L1-median">3.3</a> is a visualization of the posterior distribution, along with a linear loss function calculated for a series of possible guesses within the range of the posterior distribution. To create this visualization of the loss function, we went through the same process we described earlier for all of the guesses considered. This time, the function has the lowest value when <span class="math inline">\(g\)</span> is equal to the <strong>median</strong> of the posterior. Hence, <span class="math inline">\(L_1\)</span> is minimized at the <strong>median</strong> of the posterior one other loss function.</p>
<table>
<caption><span id="tab:L1-table">Table 3.3: </span>L1: linear loss for g = 30</caption>
<thead>
<tr class="header">
<th align="center">i</th>
<th align="center">x_i</th>
<th align="center">L1: |x_i-30|</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">4</td>
<td align="center">26</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">19</td>
<td align="center">11</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">20</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="odd">
<td align="center">14</td>
<td align="center">30</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="odd">
<td align="center">50</td>
<td align="center">47</td>
<td align="center">17</td>
</tr>
<tr class="even">
<td align="center">51</td>
<td align="center">49</td>
<td align="center">19</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center">Total</td>
<td align="center">346</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:L1-median"></span>
<img src="03-decision-03-working_files/figure-html/L1-median-1.png" alt="L1 is minimized at the median of the posterior" width="960" />
<p class="caption">
Figure 3.3: L1 is minimized at the median of the posterior
</p>
</div>
<p>If your loss function is <span class="math inline">\(L_2\)</span> (i.e. a squared loss), then the total loss for a guess is the sum of the squared differences between that guess and each value in the posterior.</p>
<p>We can once again calculate the total loss under <span class="math inline">\(L_2\)</span> if your guess is 30. Table <a href="losses-and-decision-making.html#tab:L2-table">3.4</a> summarizes the posterior distribution again, sorted in ascending order.</p>
<p>The first value is 4, and the squared difference between 4 and 30 is 676. The second value is 19, and the square of the difference between 19 and 30 is 121. The third value is 20, and the square difference between 20 and 30 is 100.</p>
<p>There is only one 30 in your posterior, and the loss for this value is 0 since it is equal to your guess. The remaining values in the posterior are again all different than 30, hence their losses are all different than 0.</p>
<p>To find the total loss, we simply sum over these individual losses again and the total loss comes out to 3,732. We have the visualization of the posterior distribution. Again, this time along with the squared loss function calculated for a possible serious of possible guesses within the range of the posterior distribution.</p>
<p>Creating the visualization in Figure <a href="losses-and-decision-making.html#fig:L2-mean">3.4</a> had the same steps. Go through the same process described earlier for a guess of 30, for all guesses considered, and record the total loss. This time, the function has the lowest value when <span class="math inline">\(g\)</span> is equal to the <strong>mean</strong> of the posterior. Hence, <span class="math inline">\(L_2\)</span> is minimized at the <strong>mean</strong> of the posterior distribution.</p>
<table>
<caption><span id="tab:L2-table">Table 3.4: </span>L2: squared loss for g = 30</caption>
<thead>
<tr class="header">
<th align="center">i</th>
<th align="center">x_i</th>
<th align="center">L2: (x_i-30)^2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">4</td>
<td align="center">676</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">19</td>
<td align="center">121</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">20</td>
<td align="center">100</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="odd">
<td align="center">14</td>
<td align="center">30</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="odd">
<td align="center">50</td>
<td align="center">47</td>
<td align="center">289</td>
</tr>
<tr class="even">
<td align="center">51</td>
<td align="center">49</td>
<td align="center">361</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center">Total</td>
<td align="center">3732</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:L2-mean"></span>
<img src="03-decision-03-working_files/figure-html/L2-mean-1.png" alt="L2 is minimized at the mean of the posterior" width="960" />
<p class="caption">
Figure 3.4: L2 is minimized at the mean of the posterior
</p>
</div>
<p>To sum up, the point estimate to report to your boss about the number of cars the dealership will sell per month <strong>depends on your loss function</strong>. In any case, you will choose to report the estimate that minimizes the loss.</p>
<ul>
<li><span class="math inline">\(L_0\)</span> is minimized at the <strong>mode</strong> of the posterior distribution.</li>
<li><span class="math inline">\(L_1\)</span> is minimized at the <strong>median</strong> of the posterior distribution.</li>
<li><span class="math inline">\(L_2\)</span> is minimized at the <strong>mean</strong> of the posterior distribution.</li>
</ul>

</div>
<div id="minimizing-expected-loss-for-hypothesis-testing" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Minimizing Expected Loss for Hypothesis Testing<a href="losses-and-decision-making.html#minimizing-expected-loss-for-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Bayesian statistics, the inference about a parameter is made based on the posterior distribution, and let’s include this in the hypothesis test setting.</p>
<p>Suppose we have two competing hypothesis, <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span>. Then we get</p>
<ul>
<li><span class="math inline">\(P(H_1 \text{ is true } | \text{ data})\)</span> = posterior probability of <span class="math inline">\(H_1\)</span></li>
<li><span class="math inline">\(P(H_2 \text{ is true } | \text{ data})\)</span> = posterior probability of <span class="math inline">\(H_2\)</span></li>
</ul>
<p>One straightforward way of choosing between <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span> would be to <strong>choose the one with the higher posterior probability</strong>. In other words, the potential decision criterion is to</p>
<ul>
<li>Reject <span class="math inline">\(H_1\)</span> if <span class="math inline">\(P(H_1 \text{ is true } | \text{ data}) &lt; P(H_2 \text{ is true } | \text{ data})\)</span>.</li>
</ul>
<p>However, since hypothesis testing is a decision problem, we should also consider a loss function. Let’s revisit the HIV testing example in Section <a href="the-basics-of-bayesian-statistics.html#sec:diagnostic-testing">1.1.2</a>, and suppose we want to test the two competing hypotheses below:</p>
<ul>
<li><span class="math inline">\(H_1\)</span>: Patient does not have HIV</li>
<li><span class="math inline">\(H_2\)</span>: Patient has HIV</li>
</ul>
<p>These are the only two possibilities, so they are mutually exclusive hypotheses that cover the entire decision space.</p>
<p>We can define the loss function as <span class="math inline">\(L(d)\)</span> – the loss that occurs when decision <span class="math inline">\(d\)</span> is made. Then the Bayesian testing procedure minimizes the posterior expected loss.</p>
<p>The possible decisions (actions) are:</p>
<ul>
<li><span class="math inline">\(d_1\)</span>: Choose <span class="math inline">\(H_1\)</span> - decide that the patient does not have HIV</li>
<li><span class="math inline">\(d_2\)</span>: Choose <span class="math inline">\(H_2\)</span> - decide that the patient has HIV</li>
</ul>
<p>For each decision <span class="math inline">\(d\)</span>, we might be right, or we might be wrong. If the decision is right, the loss <span class="math inline">\(L(d)\)</span> associated with the decision <span class="math inline">\(d\)</span> is zero, i.e. no loss. If the decision is wrong, the loss <span class="math inline">\(L(d)\)</span> associated with the decision <span class="math inline">\(d\)</span> is some positive value <span class="math inline">\(w\)</span>.</p>
<p>For <span class="math inline">\(d=d_1\)</span>, we have</p>
<ul>
<li><p><strong>Right</strong>: Decide patient does not have HIV, and indeed they do not. <span class="math inline">\(\Rightarrow L(d_1) = 0\)</span></p>
<ul>
<li><strong>Wrong</strong>: Decide patient does not have HIV, but they do. <span class="math inline">\(\Rightarrow L(d_1) = w_1\)</span></li>
</ul>
<p>For <span class="math inline">\(d=d_2\)</span>, we also have</p></li>
<li><p><strong>Right</strong>: Decide patient has HIV, and indeed they do. <span class="math inline">\(\Rightarrow L(d_2) = 0\)</span></p>
<ul>
<li><strong>Wrong</strong>: Decide patient has HIV, but they don’t <span class="math inline">\(\Rightarrow L(d2) = w_2\)</span></li>
</ul>
<p>The consequences of making a wrong decision <span class="math inline">\(d_1\)</span> or <span class="math inline">\(d_2\)</span> are different.</p></li>
</ul>
<p>Wrong <span class="math inline">\(d_1\)</span> is a <strong>false negative</strong>:</p>
<ul>
<li>We decide that patient does not have HIV when in reality they do.</li>
<li>Potential consequences: no treatment and premature death! (severe)</li>
</ul>
<p>Wrong <span class="math inline">\(d_2\)</span> is a <strong>false positive</strong>:</p>
<ul>
<li>We decide that the patient has HIV when in reality they do not.</li>
<li>Potential consequences: distress and unnecessary further investigation. (not ideal but less severe than the consequences of a false negative decision)</li>
</ul>
<p>Let’s put these definitions in the context of the HIV testing example with ELISA.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li><span class="math inline">\(H_1\)</span>: Patient does not have HIV</li>
<li><span class="math inline">\(H_2\)</span>: Patient has HIV</li>
</ul>
<p><strong>Decision</strong></p>
<ul>
<li><span class="math inline">\(d_1\)</span>: Choose <span class="math inline">\(H_1\)</span> - decide that the patient does not have HIV</li>
<li><span class="math inline">\(d_2\)</span>: Choose <span class="math inline">\(H_2\)</span> - decide that the patient has HIV</li>
</ul>
<p><strong>Losses</strong></p>
<ul>
<li><p><span class="math inline">\(L(d_1) = \left\{ \begin{array}{cc} 0 &amp; \text{if $d_1$ is right}\\ w_1=1000 &amp; \text{if $d_1$ is wrong} \end{array}\right.\)</span></p></li>
<li><p><span class="math inline">\(L(d_2) = \left\{ \begin{array}{cc} 0 &amp; \text{if $d_2$ is right}\\ w_2=10 &amp; \text{if $d_2$ is wrong} \end{array}\right.\)</span></p></li>
</ul>
<p>The values of <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are arbitrarily chosen. But the important thing is that <span class="math inline">\(w_1\)</span>, the loss associated with a false negative determination, is much higher than <span class="math inline">\(w_2\)</span>, the loss associated with a false positive determination.</p>
<p><strong>Posteriors</strong></p>
<p>The plus sign means that our patient had tested positive on the ELISA.</p>
<ul>
<li><span class="math inline">\(P(H_1|+) \approx 0.88\)</span> - the posterior probability of the patient <strong>not</strong> having HIV given positive ELISA result</li>
<li><span class="math inline">\(P(H_2|+) \approx 0.12\)</span> - the posterior probability of the patient having HIV given positive ELISA result, as the complement value of <span class="math inline">\(P(H_1|+)\)</span></li>
</ul>
<p><strong>Expected losses</strong></p>
<ul>
<li><span class="math inline">\(E[L(d_1)] = 0.88 \times 0 + 0.12 \times 1000 = 120\)</span></li>
<li><span class="math inline">\(E[L(d_2)] = 0.88 \times 10 + 0.12 \times 0 = 8.8\)</span></li>
</ul>
<p>Since the expected loss for <span class="math inline">\(d_2\)</span> is lower, we should make this decision – the patient has HIV.</p>
<p>Note that our decision is highly influenced by the losses we assigned to <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span>.</p>
<p>If the losses were symmetric, say <span class="math inline">\(w_1 = w_2 = 10\)</span>, then the expected loss for <span class="math inline">\(d_1\)</span> becomes</p>
<p><span class="math display">\[E[L(d_1)] = 0.88 \times 0 + 0.12 \times 10 = 1.2,\]</span></p>
<p>while the expected loss for <span class="math inline">\(d_2\)</span> would not change. Therefore, we would choose <span class="math inline">\(d_1\)</span> instead; that is, we would decide that the patient does not have HIV.</p>
<p>To recap, Bayesian methodologies allow for the integration of losses into the decision making framework easily. And in Bayesian testing, we minimize the posterior expected loss.</p>

</div>
<div id="sec:bayes-factors" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Posterior Probabilities of Hypotheses and Bayes Factors<a href="losses-and-decision-making.html#sec:bayes-factors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we will continue with the HIV testing example to introduce the concept of Bayes factors. Earlier, we introduced the concept of priors and posteriors. The <strong>prior odds</strong> is defined as <strong>the ratio of the prior probabilities of hypotheses</strong>.</p>
<p>Therefore, if there are two competing hypotheses being considered, then the prior odds of <span class="math inline">\(H_1\)</span> to <span class="math inline">\(H_2\)</span> can be defined as <span class="math inline">\(O[H_1:H_2]\)</span>, which is equal to <span class="math inline">\(P(H_1)\)</span> over probability of <span class="math inline">\(P(H_2)\)</span>. In mathematical terms,</p>
<p><span class="math display">\[O[H_1:H_2] = \frac{P(H_1)}{P(H_2)}\]</span></p>
<p>Similarly, the <strong>posterior odds</strong> is <strong>the ratio of the two posterior probabilities of hypotheses</strong>, written as</p>
<p><span class="math display">\[PO[H_1:H_2] = \frac{P(H_1|\text{data})}{P(H_2|\text{data})}\]</span></p>
<p>Using Bayes’ rule, we can rewrite the posterior probabilities as below:</p>
<p><span class="math display">\[\begin{aligned}
PO[H_1:H_2] &amp;= \frac{P(H_1|\text{data})}{P(H_2|\text{data})} \\
&amp;= \frac{(P(\text{data}|H_1) \times P(H_1)) / P(\text{data}))}{(P(\text{data}|H_2) \times P(H_2)) / P(\text{data}))} \\
&amp;= \frac{(P(\text{data}|H_1) \times P(H_1))}{(P(\text{data}|H_2) \times P(H_2))} \\
&amp;= \boxed{\frac{P(\text{data}|H_1)}{P(\text{data}|H_2)}} \times \boxed{\frac{P(H_1)}{P(H_2)}} \\
&amp;= \textbf{Bayes factor} \times \textbf{prior odds}
\end{aligned}\]</span></p>
<p>In mathematical notation, we have</p>
<p><span class="math display">\[PO[H_1:H_2] = BF[H_1:H_2] \times O[H_1:H_2]\]</span></p>
<p>In other words, the posterior odds is the product of the Bayes factor and the prior odds for these two hypotheses.</p>
<p>The Bayes factor quantifies the evidence of data arising from <span class="math inline">\(H_1\)</span> versus <span class="math inline">\(H_2\)</span>.</p>
<p>In a discrete case, the Bayes factor is simply the ratio of the likelihoods of the observed data under the two hypotheses, written as</p>
<p><span class="math display">\[BF[H_1:H_2] = \frac{P(\text{data}|H_1)}{P(\text{data}|H_2)}.\]</span></p>
<p>On the other hand, in a continuous case, the Bayes factor is the ratio of the marginal likelihoods, written as</p>
<p><span class="math display">\[BF[H_1:H_2] = \frac{\int P(\text{data}|\theta,H_1)d\theta}{\int P(\text{data}|\theta,H_2)d\theta}.\]</span></p>
<p>Note that <span class="math inline">\(\theta\)</span> is the set formed by all possible values of the model parameters.</p>
<p>In this section, we will stick with the simpler discrete case. And in upcoming sections, we will revisit calculating Bayes factors for more complicated models.</p>
<p>Let’s return to the HIV testing example from earlier, where our patient had tested positive in the ELISA.</p>
<p><strong>Hypotheses</strong></p>
<ul>
<li><span class="math inline">\(H_1\)</span>: Patient does not have HIV</li>
<li><span class="math inline">\(H_2\)</span>: Patient has HIV</li>
</ul>
<p><strong>Priors</strong></p>
<p>The prior probabilities we place on these hypothesis came from the prevalence of HIV at the time in the general population. We were told that the prevalence of HIV in the population was 1.48 out of 1000, hence the prior probability assigned to <span class="math inline">\(H_2\)</span> is 0.00148. And the prior assigned to <span class="math inline">\(H_1\)</span> is simply the complement of this.</p>
<ul>
<li><p><span class="math inline">\(P(H_1) = 0.99852\)</span> and <span class="math inline">\(P(H_2) = 0.00148\)</span></p>
<p>The prior odds are</p></li>
<li><p><span class="math inline">\(O[H_1:H_2] = \dfrac{P(H_1)}{P(H_2)} = \dfrac{0.99852}{0.00148} = 674.6757\)</span></p>
<p><strong>Posteriors</strong></p>
<p>Given a positive ELISA result, the posterior probabilities of these hypotheses can also be calculated, and these are approximately 0.88 and 0.12. We will hold on to more decimal places in our calculations to avoid rounding errors later.</p></li>
<li><p><span class="math inline">\(P(H_1|+) = 0.8788551\)</span> and <span class="math inline">\(P(H_2|+) = 0.1211449\)</span></p>
<p>The posterior odds are</p></li>
<li><p><span class="math inline">\(PO[H_1:H_2] = \dfrac{P(H_1|+)}{P(H_2|+)} = \dfrac{0.8788551}{0.1211449} = 7.254578\)</span></p>
<p><strong>Bayes Factor</strong></p>
<p>Finally, we can calculate the Bayes factor as the ratio of the posterior odds to prior odds, which comes out to approximately 0.0108. Note that in this simple discrete case the Bayes factor, it simplifies to the ratio of the likelihoods of the observed data under the two hypotheses.</p></li>
</ul>
<p><span class="math display">\[\begin{aligned}
BF[H_1:H_2] &amp;= \frac{PO[H_1:H_2]}{O[H_1:H_2]} = \frac{7.25457}{674.6757} \approx 0.0108 \\
&amp;= \frac{P(+|H_1)}{P(+|H_2)} = \frac{0.01}{0.93} \approx 0.0108
\end{aligned}\]</span></p>
<p>Alternatively, remember that the true positive rate of the test was 0.93 and the false positive rate was 0.01. Using these two values, the Bayes factor also comes out to approximately 0.0108.</p>
<p>So now that we calculated the Bayes factor, the next natural question is, what does this number mean? A commonly used scale for interpreting Bayes factors is proposed by <span class="citation">Jeffreys (<a href="#ref-jeffreys1961theory" role="doc-biblioref">1961</a>)</span>, as in Table <a href="losses-and-decision-making.html#tab:jeffreys1961">3.5</a>. If the Bayes factor is between 1 and 3, the evidence against <span class="math inline">\(H_2\)</span> is not worth a bare mention. If it is 3 to 20, the evidence is positive. If it is 20 to 150, the evidence is strong. If it is greater than 150, the evidence is very strong.</p>
<table>
<caption><span id="tab:jeffreys1961">Table 3.5: </span>Interpreting the Bayes factor</caption>
<thead>
<tr class="header">
<th align="center">BF[H_1:H_2]</th>
<th align="center">Evidence against H_2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1 to 3</td>
<td align="center">Not worth a bare mention</td>
</tr>
<tr class="even">
<td align="center">3 to 20</td>
<td align="center">Positive</td>
</tr>
<tr class="odd">
<td align="center">20 to 150</td>
<td align="center">Strong</td>
</tr>
<tr class="even">
<td align="center">&gt; 150</td>
<td align="center">Very strong</td>
</tr>
</tbody>
</table>
<p>It might have caught your attention that the Bayes factor we calculated does not even appear on the scale. To obtain a Bayes factor value on the scale, we will need to change the order of our hypotheses and calculate <span class="math inline">\(BF[H_2:H_1]\)</span>, i.e. the Bayes factor for <span class="math inline">\(H_2\)</span> to <span class="math inline">\(H_1\)</span>. Then we look for evidence against <span class="math inline">\(H_1\)</span> instead.</p>
<p>We can calculate <span class="math inline">\(BF[H_2:H_1]\)</span> as a reciprocal of <span class="math inline">\(BF[H_1:H_2]\)</span> as below:</p>
<p><span class="math display">\[BF[H_2:H_1] = \frac{1}{BF[H_1:H_2]} = \frac{1}{0.0108} = 92.59259\]</span></p>
<p>For our data, this comes out to approximately 93. Hence the evidence against <span class="math inline">\(H_1\)</span> (the patient does not have HIV) is strong. Therefore, even though the posterior for having HIV given a positive result, i.e. <span class="math inline">\(P(H_2|+)\)</span>, was low, we would still decide that the patient has HIV, according to the scale based on a positive ELISA result.</p>
<p>An intuitive way of thinking about this is to consider not only the posteriors, but also the priors assigned to these hypotheses. Bayes factor is the ratio of the posterior odds to prior odds. While 12% is a low posterior probability for having HIV given a positive ELISA result, this value is still much higher than the overall prevalence of HIV in the population (in other words, the prior probability for that hypothesis).</p>
<p>Another commonly used scale for interpreting Bayes factors is proposed by <span class="citation">Kass and Raftery (<a href="#ref-kass1995bayes" role="doc-biblioref">1995</a>)</span>, and it deals with the natural logarithm of the calculated Bayes factor. The values can be interpreted in Table <a href="losses-and-decision-making.html#tab:kass1995">3.6</a>.</p>
<table>
<caption><span id="tab:kass1995">Table 3.6: </span>Interpreting the Bayes factor</caption>
<thead>
<tr class="header">
<th align="center">2*log(BF[H_2:H_1])</th>
<th align="center">Evidence against H_1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0 to 2</td>
<td align="center">Not worth a bare mention</td>
</tr>
<tr class="even">
<td align="center">2 to 6</td>
<td align="center">Positive</td>
</tr>
<tr class="odd">
<td align="center">6 to 10</td>
<td align="center">Strong</td>
</tr>
<tr class="even">
<td align="center">&gt; 10</td>
<td align="center">Very strong</td>
</tr>
</tbody>
</table>
<p>Reporting of the log scale can be helpful for numerical accuracy reasons when the likelihoods are very small. Taking two times the natural logarithm of the Bayes factor we calculated earlier, we would end up with the same decision that the evidence against <span class="math inline">\(H_1\)</span> is strong.</p>
<p><span class="math display">\[2 \times \log(92.59259) = 9.056418\]</span></p>
<p>To recap, we defined prior odds, posterior odds, and the Bayes factor. We learned about scales by which we can interpret these values for model selection. We also re-emphasize that in Bayesian testing, the order in which we evaluate the models of hypotheses does <strong>not</strong> matter. The Bayes factor of <span class="math inline">\(H_2\)</span> versus <span class="math inline">\(H_1\)</span>, <span class="math inline">\(BF[H_2:H_1]\)</span>, is simply the reciprocal of the Bayes factor for <span class="math inline">\(H_1\)</span> versus <span class="math inline">\(H_2\)</span>, that is, <span class="math inline">\(BF[H_1:H_2]\)</span>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-jeffreys1961theory" class="csl-entry">
Jeffreys, Sir Harold. 1961. <em>Theory of Probability: 3rd Edition</em>. Clarendon Press.
</div>
<div id="ref-kass1995bayes" class="csl-entry">
Kass, Robert E, and Adrian E Raftery. 1995. <span>“Bayes Factors.”</span> <em>Journal of the American Statistical Association</em> 90 (430): 773–95.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference-and-decision-making-with-multiple-parameters.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/statswithr/book/edit/master/03-decision-00-intro.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
